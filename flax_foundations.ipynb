{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frMdnYcLnLF-"
   },
   "source": [
    "### Flax Foundations\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/rastringer/jax_notebooks/blob/master/flax_foundations.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Efficient and flexible model development\n",
    "\n",
    "By combining JAX's auto-differentiation and Flax's modular design, developers can easily construct and train state-of-the-art deep learning models. JAX/Flax traces pure functions and compiles for GPU and TPU accelerators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1685537268919,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "XmmqFUv_nU-O"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1148,
     "status": "ok",
     "timestamp": 1685537270064,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "Hr3w7WMmnY4I",
    "outputId": "53f53e28-21a5-4bc0-e148-34612437e89d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([-1.3721193 ,  0.61131495,  0.6442836 ,  2.2192965 , -1.1271116 ],      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a single dense layer that takes a number of features as input\n",
    "model = nn.Dense(features=5)\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "# Dummy input data\n",
    "x = random.normal(key1, (10,))\n",
    "# Initialize the model\n",
    "params = model.init(key2, x)\n",
    "# Forward pass\n",
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWV5_FaBxHkW"
   },
   "source": [
    "Note we only mention to Flax the number of features for the output of the model, rather than specifying the size of the input. Flax works out the correct kernel size for us!\n",
    "\n",
    "Let's take a look at the pytree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1685483747199,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "9fZjIiJmxb_P",
    "outputId": "c5a738cc-d5fe-4810-da58-231e4d62e445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: (5,),\n",
       "        kernel: (10, 5),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output shapes\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oPDRcYkxomP"
   },
   "source": [
    "Notice the parameters are stored in a `FrozenDict`, which prevents any mutation of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1505,
     "status": "ok",
     "timestamp": 1685483756824,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "IyGZlYexppJ0",
    "outputId": "6ae0a142-37b6-41f8-f508-6c93d1347f65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    params: {\n",
      "        dense1: {\n",
      "            bias: (16,),\n",
      "            kernel: (3, 16),\n",
      "        },\n",
      "        dense2: {\n",
      "            bias: (1,),\n",
      "            kernel: (16, 1),\n",
      "        },\n",
      "    },\n",
      "})\n",
      "Inputs: \n",
      "[[0.2 0.3 0.4]\n",
      " [0.1 0.2 0.3]]\n",
      "\n",
      "Predictions: \n",
      "[[-0.01026188]\n",
      " [-0.01458298]]\n",
      "\n",
      "Target data: \n",
      "[[0.5]\n",
      " [0.8]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "# Dummy data\n",
    "inputs = jnp.array([[0.2, 0.3, 0.4], [0.1, 0.2, 0.3]])\n",
    "targets = jnp.array([[0.5], [0.8]])\n",
    "\n",
    "# Simple feedforward neural network\n",
    "class SimpleNetwork(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(self.hidden_size)\n",
    "        self.dense2 = nn.Dense(self.output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "# Initialization\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "rng = jax.random.PRNGKey(0)\n",
    "model = SimpleNetwork(hidden_size, output_size)\n",
    "params = model.init(rng, inputs)\n",
    "tree = jax.tree_util.tree_map(lambda inputs: inputs.shape, params) # Checking output shapes\n",
    "print(tree)\n",
    "\n",
    "# Forward pass\n",
    "predictions = model.apply(params, inputs)\n",
    "\n",
    "print(f\"Inputs: \\n{inputs}\")\n",
    "print(f\"\\nPredictions: \\n{predictions}\")\n",
    "print(f\"\\nTarget data: \\n{targets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d41NyqQ_5f2K"
   },
   "source": [
    "In this example, we defined our model explicitly using `setup`. We can also define architecrures using `nn.compact`, which allows us to define a modulea s a single method. This can lead to cleaner code if you are writing custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5YxaUOt53qt"
   },
   "source": [
    "Here's our SimpleNetwork again, using `setup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifHEHhCRmJSy"
   },
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(self.hidden_size)\n",
    "        self.dense2 = nn.Dense(self.output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFZLcHHm6FE7"
   },
   "source": [
    "And using `nn.compact`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-EtWXv46HjL"
   },
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "  hidden_size: int\n",
    "  output_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Dense(hidden_size, name=\"dense1\")(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(output_size, name=\"dense2\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeVz2Hd76j_3"
   },
   "source": [
    "If you are porting models from PyTorch, or prefer explicit definition and separation of submodules, `setup` may suit. `nn.compact` may be best for reducing duplication, writing code that looks closer to mathematical notation, or if you are using shape inference (parameters dependant on shapes of inputs unknown at initialization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnfpZpfw4_Zl"
   },
   "source": [
    "### Flax modules\n",
    "\n",
    "Flax it easy to incorporate training techniques such as batch normalization and learning rate scheduling via the `flax.linen.Module`.\n",
    "\n",
    "Here's our simple multi-layer perceptron again:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JI6G6gh7MCs"
   },
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "  hidden_size: int\n",
    "  output_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Dense(hidden_size, name=\"dense1\")(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(output_size, name=\"dense2\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj7P1lwO9D6D"
   },
   "source": [
    "Batch normalization is a regularization technique which computes running averages over feature dimensions. This speeds up training cycles and improves convergence.\n",
    "To apply batch normalization, we call upon `flax.linen.BatchNorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwMMDB0x84yd"
   },
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "  hidden_size: int\n",
    "  output_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, train: bool):\n",
    "    x = nn.Dense(hidden_size, name=\"dense1\")(x)\n",
    "    x = nn.BatchNorn(use_running_average=not train)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(output_size, name=\"dense2\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJwRGivN9oLf"
   },
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout is another (stochastic) regularization technique that randomly removes units in a network to improve reduce overfitting and improve generalization.\n",
    "\n",
    "Dropout requires our PRNG skills to endure it is a random operation.\n",
    "\n",
    "When splitting a key, we can simply split into three keys, granting the third for `flax.linen.dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wd_Mg6y596ba"
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(seed=0)\n",
    "main_key, params_key, dropout_key = jax.random.split(key=key, num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFmik74I-azQ"
   },
   "source": [
    "Then add the module to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2wML9NC-XAO"
   },
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "  hidden_size: int\n",
    "  output_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, train: bool):\n",
    "    x = nn.Dense(hidden_size, name=\"dense1\")(x)\n",
    "    x = nn.Dropout(rate=0.5, deterministic=not train)(x)\n",
    "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(output_size, name=\"dense2\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJxslb8W_Oct"
   },
   "source": [
    "We can then initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_Biu5lb_QFQ"
   },
   "outputs": [],
   "source": [
    "simple_net = SimpleNetwork(hidden_size=5, output_size=1)\n",
    "x = jnp.empty((3, 4, 4, 5, 5))\n",
    "# Dropout is enabled via `deterministic=True`.\n",
    "variables = simple_net.init(params_key, x, train=False)\n",
    "params = variables['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-B4GjBN-6ia"
   },
   "source": [
    "### Train states\n",
    "\n",
    "A \"train state\" is the mutable state of a model during training, including properties such as its parameters (weights) and optimizer state.\n",
    "\n",
    "The train state is typically represented as an instance of the `flax.training.TrainState` class, which encapsulates and provides methods to update the state.\n",
    "\n",
    "One of the features of JAX/Flax is its functional programming characteristic of immutability. Models are updates are purely functional, enabling model parallelism and efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YZ0SQdvCas8"
   },
   "outputs": [],
   "source": [
    "# Example, will not run\n",
    "\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  cnn = CNN()\n",
    "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  tx = optax.sgd(learning_rate, momentum)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=cnn.apply, params=params, tx=tx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGMROXwXCXs9"
   },
   "source": [
    "### Optax\n",
    "\n",
    "[Optax](https://optax.readthedocs.io/en/latest/) is a gradient processing and optimization package. It is generally used with Flax as follows:\n",
    "\n",
    "Create an optimizer state from parameters using any optimization method (eg `optax.rmsprop`).\n",
    "Compute loss gradients using `value_and_grad()`.\n",
    "Call the Optax `update` function to update the internal optimizer state to work out how to tweak the parameters. Use `apply_updates` to apply update the to the parameters.\n",
    "\n",
    "For example (will not run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLcjVhvbAwlY"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "optimizer = optax.adam(learning_rate=learning_rate)\n",
    "optimizer_state = optimizer.init(params)\n",
    "loss_grad_func = jax.value_and_grad(mse)\n",
    "\n",
    "for i in range(10):\n",
    "  loss, grads = loss_grad_func(params, x_samples, y_samples)\n",
    "  updates, optimizer_state = optimizer.update(grads, optimizer_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  if i % 10 == 0:\n",
    "    print('Loss step {}: '.format(i), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_bga5KCkBoR"
   },
   "source": [
    "MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sy_ZRDIger5y"
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "from flax import linen as nn\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import train_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, images, labels):\n",
    "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn({'params': params}, images)\n",
    "    one_hot = jax.nn.one_hot(labels, 10)\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "    return loss, logits\n",
    "\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(state.params)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  return grads, loss, accuracy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "  return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "def train_epoch(state, train_ds, batch_size, rng):\n",
    "  \"\"\"Train for a single epoch.\"\"\"\n",
    "  train_ds_size = len(train_ds['image'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "  epoch_loss = []\n",
    "  epoch_accuracy = []\n",
    "\n",
    "  for perm in perms:\n",
    "    batch_images = train_ds['image'][perm, ...]\n",
    "    batch_labels = train_ds['label'][perm, ...]\n",
    "    grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
    "    state = update_model(state, grads)\n",
    "    epoch_loss.append(loss)\n",
    "    epoch_accuracy.append(accuracy)\n",
    "  train_loss = np.mean(epoch_loss)\n",
    "  train_accuracy = np.mean(epoch_accuracy)\n",
    "  return state, train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def get_datasets():\n",
    "  \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "  ds_builder = tfds.builder('mnist')\n",
    "  ds_builder.download_and_prepare()\n",
    "  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "  train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "  test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "  return train_ds, test_ds\n",
    "\n",
    "\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  cnn = CNN()\n",
    "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  tx = optax.sgd(learning_rate, momentum)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=cnn.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "def train_and_evaluate(learning_rate, momentum,\n",
    "                       batch_size, num_epochs) -> train_state.TrainState:\n",
    "  \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "  Args:\n",
    "    config: Hyperparameter configuration for training and evaluation.\n",
    "    workdir: Directory where the tensorboard summaries are written to.\n",
    "\n",
    "  Returns:\n",
    "    The train state (which includes the `.params`).\n",
    "  \"\"\"\n",
    "  train_ds, test_ds = get_datasets()\n",
    "  rng = jax.random.PRNGKey(0)\n",
    "\n",
    "  rng, init_rng = jax.random.split(rng)\n",
    "  state = create_train_state(init_rng, 0.01, 0.9 )\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state, train_loss, train_accuracy = train_epoch(state, train_ds,\n",
    "                                                    64,\n",
    "                                                    input_rng)\n",
    "    _, test_loss, test_accuracy = apply_model(state, test_ds['image'],\n",
    "                                              test_ds['label'])\n",
    "\n",
    "    print(\n",
    "        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n",
    "        % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
    "           test_accuracy * 100))\n",
    "\n",
    "    print('train_loss', train_loss, epoch)\n",
    "    print('train_accuracy', train_accuracy, epoch)\n",
    "    print('test_loss', test_loss, epoch)\n",
    "    print('test_accuracy', test_accuracy, epoch)\n",
    "\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM_B4krKepfM"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(0.01, 0.9, 128, 1)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMZNwo4ZPt+hFDrNNiRH2qh",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
