[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Turbocharge ML with JAX and TPUs",
    "section": "",
    "text": "Welcome\nJAX and TPUs are powering developers and researchers to speed up machine learning training workloads and have more fun in the process.\nYou’ll learn JAX’s elegant, functional style and transformations to super charge code that will look familiar to anyone who has worked with Python and NumPy.  We will build upon taking derivatives and just-in-time compilation to linear regression and basic image recognition models.\nIn later stages of the course, we will run cutting edge models including Stable Diffusion from Hugging Face, and Google Research’s Vision Transformer.\nWe hope you enjoy the course and start experimenting!"
  },
  {
    "objectID": "TPUs.html",
    "href": "TPUs.html",
    "title": "1  TPUs",
    "section": "",
    "text": "1.0.1 Supercomputer for ML\nGoogle designed Cloud TPUs as a matrix processor focused making training and inference of neural networks faster, and more power efficient. The TPU is built for massive matrix processing, and its systolic array architecture assigns thousands of interconnected multiply-accumulators to the task. Cloud TPU v3 contains two systolic arrays of 128 x 128 ALUs, on a single processor. For workloads bound by matmul, TPU can generate significant efficiencies.\n\n\n1.0.2 Getting started\nPrerequisite: a Google Cloud project.\nThere are several ways to run the commands below.\nVertex AI: If you’re running notebooks from within Vertex Workbench, simply open a terminal within the notebook instance and run the commands.  Local: Download the gcloud SDK, or open a shell from within Cloud console. * Compute Engine VM: run the commands to set up a TPU VM.\n\n\n1.0.3 Setting up the VM\nFirst, run the following command to enable the TPU API, and set your user and project configuration:\ngcloud services enable tpu.googleapis.com\ngcloud config set account &lt;your-email-account&gt;\ngcloud config set project &lt;your-project&gt;\n\n\n1.0.4 Create the TPU VM\nFor more information, an extensive guide can be found here.\ngcloud compute tpus tpu-vm create tpu-name  \\\n  --zone=zone \\\n  --accelerator-type=${ACCELERATOR_TYPE}  \\\n  --version=tpu-vm-v4-base\nNote: For v2 and v3 configurations use the tpu-vm-base TPU software version. For v4 configurations use tpu-vm-v4-base. The correct version of libtpu.so is automatically installed when JAX is installed on the machine.\nSince there is no TPU specific JAX software version, we have to manually install JAX on the TPU VM.\nTo see a list of versions (such as TensorFlow, other PyTorch versions), replace with zone with the zone of yoru project (eg us-central1-b) and run:\ngcloud compute tpus tpu-vm versions list --zone &lt;ZONE&gt;\nFor all TPU types, the version is followed by the number of TensorCores (e.g., 8, 32, 128). For example, –accelerator-type=v2-8 specifies a TPU v2 with 8 TensorCores and v3-1024 specifies a v3 TPU with 1024 TensorCores (a slice of a v3 Pod).\n\n\n1.0.5 Connecting to a TPU VM\nFrom one of the options above (Workbench, local terminal, Compute Engine VM etc), adjust the VM name and zone placeholders:\ngcloud compute tpus tpu-vm ssh &lt;your-tpu-vm-name&gt; --zone &lt;your-zone&gt;\n\n\n1.0.6 Connecting to a TPU VM via local notebook\nOne of the most popular ways to connect is via a Jupyter Notebook either on another VM or a local machine. This means that rather than develop in notebooks and move .py files to the TPU VM to run them, all experimentation on the notebook can benefit from the TPU.\n\n1.0.6.1 Steps:\nSet up the TPU VM as above, and connect from your local machine (or another VM) with a slightly different command (change the parameters within &lt;...&gt;):\ngcloud compute tpus tpu-vm ssh &lt;tpu_vm_name&gt; --zone &lt;zone&gt;  -- -L 8888:localhost:8888\nOnce connected for the first time, install the JAX library:\npip install --upgrade 'jax[tpu]&gt;0.3.0' \\\n  -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\"\nLet’s check JAX is working before going further. From the terminal connected to the VM, try the following lines of code (number of devices will vary depending on configuration):\npython3\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;  num_devices = jax.device_count()\n&gt;&gt;&gt; device_type = jax.devices()[0].device_kind\n&gt;&gt;&gt; print(f\"Using {num_devices} JAX devices of type {device_type}.\")\n&gt;&gt;&gt; Using 8 JAX devices of type Cloud TPU.\nIt should then be possible to launch a notebook running on the TPU via the usual command:\njupyter-lab\nor\njupyter notebook\ndepending on whether you have JupyterLab or classic Jupyter Notebook installed. Now accessing localhost:8888 in a browser (use the link in the terminal that results from the commands above) should take you to the notebook environment.\n\n\n\n1.0.7 Connecting to a TPU VM\nFrom one of the options above (Workbench, local terminal, Compute Engine VM etc), adjust the VM name and zone placeholders:\ngcloud compute tpus tpu-vm ssh &lt;your-tpu-vm-name&gt; --zone &lt;your-zone&gt;\n\n1.0.7.1 Connecting to a TPU VM via local notebook\nOne of the most popular ways to connect is via a Jupyter Notebook either on another VM or a local machine. This means that rather than develop in notebooks and move .py files to the TPU VM to run them, all experimentation on the notebook can benefit from the TPU.\n\n\n1.0.7.2 Steps:\nSet up the TPU VM as above, and connect from your local machine (or another VM) with a slightly different command (change the parameters within &lt;...&gt;):\ngcloud compute tpus tpu-vm ssh &lt;tpu_vm_name&gt; --zone &lt;zone&gt;  -- -L 8888:localhost:8888\nIt should then be possible to launch a notebook running on the TPU via the usual command:\njupyter-lab\nor\njupyter notebook\ndepending on whether you have JupyterLab or classic Jupyter Notebook installed. Now accessing localhost:8888 in a browser (use the link in the terminal that results from the commands above) should take you to the notebook environment."
  },
  {
    "objectID": "JAX_foundations.html#transformations",
    "href": "JAX_foundations.html#transformations",
    "title": "2  Introduction to JAX",
    "section": "2.1 Transformations",
    "text": "2.1 Transformations\nModular, functional programming\nJAX can transform functions. This means a numerical function can be returned as a new function that, for example, computes the gradient of, or parallelizes the original function. It could also do both!\n\n2.1.1 grad\nOne of the most commonly used transformations, jax.grad calculates the gradient of a function.\n\nfrom jax import grad\n\ndef sum_squares(x):\n    return jnp.sum(x**2)\n\nSince jax.grad(f) computes the gradient of function f, jax.grad(f)(x) is the gradient of f at x .\n\nprint(grad(sum_squares)(3.0))\n\n6.0\n\n\n\nprint(grad(grad(sum_squares))(3.0))\n\n2.0\n\n\n\nimport math\n\ndef cylinder_volume(r, h):\n    vol = jnp.pi * r**2 * h\n    return vol\n\n# Compute the volume of a cylinder with radius 3, and height 3\nprint(cylinder_volume(3, 3))\n\n84.82300164692441\n\n\n\nprint(grad(cylinder_volume)(4.0, 8.0))\nprint(grad(cylinder_volume)(2.0, 6.0))\n\n201.06194\n75.398224\n\n\n\nprint(grad(grad(cylinder_volume))(4.0, 8.0))\nprint(grad(grad(cylinder_volume))(2.0, 6.0))\n\n50.265484\n37.699112\n\n\nWe can use argnums to calculate the gradient with respect to different arguments:\n\ndef f(x):\n  if x &gt; 0:\n    return 2 * x ** 3\n  else:\n    return 3 * x\n\n\nkey = random.PRNGKey(0)\nx = random.normal(key, ())\nprint(key)\nprint(x)\n\nprint(grad(f)(x))\nprint(grad(f)(-x))\n\n[0 0]\n-0.20584226\n3.0\n0.2542262\n\n\nAn obvious example to make use of grad would be a loss function.\n\ndef loss(preds, targets):\n  return jnp.sum((preds-targets)**2)\n\nx = jnp.asarray([1.0, 2.0, 3.0, 4.0])\ntargets = jnp.asarray([1.1, 2.1, 3.1, 4.1])\n\nprint(grad(loss)(x, y))\n\n[-4. -2.  0.  2.]\n\n\n\n\n2.1.2 Value and grad\nWe can return both the value and gradient of a function using value_and_grad. This is a common pattern in machine learning for logging training loss.\n\nfrom jax import value_and_grad\n\nvalue_and_grad(loss)(x, y)\n\n(Array(6., dtype=float32), Array([-4., -2.,  0.,  2.], dtype=float32))\n\n\n\n\n2.1.3 jit\nThe jax.jit() transformation performs Just In Time (JIT) compilation of a JAX Python function for efficient execution in XLA.\nLet’s go back to our sum_squares() function and time its original implementation on an array of numbers 1-100.\n\nfrom jax import jit\n\ndef sum_squares(x):\n    return jnp.sum(x**2)\n\nx = jnp.arange(100)\n\n%timeit sum_squares(x).block_until_ready()\n\n419 µs ± 41.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\nLet’s jit the function and notice the speed improvement.\n\nsum_squares_jit = jit(sum_squares)\n\n# Warm up\nsum_squares_jit(x).block_until_ready()\n\n%timeit sum_squares_jit(x).block_until_ready()\n\n72.6 µs ± 20.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\nIn case this notation isn’t familiar, µs denotes a ‘microsecond’, or a millionth of a second. ns is a ‘nanosecond’, a billionth of a second. Our jitted function is considerably faster in this simple example. Note: JAX’s asynchronous execution model means the Python call might return before the computation ends. This is why we use the block_until_ready() method to make sure we return the end result. a returned array would not be populated as soon as the function returns. Using block_until_ready means we time the actual computation, not just the dispatch.\n\n\n2.1.4 Sharp edges\nIt isn’t possible or economical to jit everything. jit will throw errors when function inputs spark conditional chains (eg if x &lt; 5: … ) and jit itself creates some overhead. jit is best reserved for compiling complex functions that will run several times, such as updating weights in a training loop.\n\n\n2.1.5 vmap\nThe jax.vmap transformation generates a vectorized implementation of a function.\nReference for this section (thanks to DeepMind).\nWe can loop over a batch in Python however such operations tend to be costly.\n\nfrom jax import vmap\n\nmat = random.normal(key, (150, 100))\nbatched_x = random.normal(key, (10, 100))\n\ndef apply_matrix(v):\n  return jnp.dot(mat, v)\n\n\ndef naively_batched_apply_matrix(v_batched):\n  return jnp.stack([apply_matrix(v) for v in v_batched])\n\nprint('Naively batched')\n%timeit naively_batched_apply_matrix(batched_x).block_until_ready()\n\nNaively batched\n3.62 ms ± 298 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\ndef vmap_batched_apply_matrix(v_batched):\n  return vmap(apply_matrix)(v_batched)\n\nprint('Auto-vectorized with vmap')\n%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()\n\nAuto-vectorized with vmap\n1.17 ms ± 21.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n\n@jit\ndef jit_vmap_batched_apply_matrix(v_batched):\n  return vmap(apply_matrix)(v_batched)\n\nprint('jitted and auto-vectorized with vmap')\n%timeit jit_vmap_batched_apply_matrix(batched_x).block_until_ready()\n\njitted and auto-vectorized with vmap\n86.9 µs ± 10 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\nPutting it all together\nWe take a loss function, use it to find gradients with grad, vectorize it for work across batches, then jit compile, all in one line.\n\nimport jax.numpy as jnp\nfrom jax import grad, vmap, jit\n\ndef predict(params, inputs):\n    for W, b in params:\n        outputs = jnp.dot(inputs, W) + b\n        inputs = jnp.tahn(outputs)\n    return outputs\n\ndef mse_loss(params, batch):\n    inputs, targets = batch\n    preds = predict(params, inputs)\n    loss = jnp.sum((preds - targets) ** 2)\n    print(loss)\n    return loss\n\ngradients = jit(grad(mse_loss))\nvectorized_gradients = jit(vmap(grad(mse_loss), in_axes=(None, 0)))\n\n\n\n2.1.6 pmap\npmap\n\ndef pmap_batched_apply_matrix(v_batched):\n  return pmap(apply_matrix)(v_batched)\n\n\npmap_batched_apply_matrix(batched_x)\n\nNameError: ignored"
  },
  {
    "objectID": "jax_linear_regression.html",
    "href": "jax_linear_regression.html",
    "title": "3  Linear Regression in JAX",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom jax import tree_util, random\n\n  \nReference: JAX for the impatient\n\n3.0.1 Pytrees\nBefore we jump into linear regression in JAX, let’s have a quick look at pytrees. Pytrees are everywhere in JAX and Flax.\nJAX treats a pytree as a container of leaf elements. These can include lists, tuples and dicts, so is basically a structure for nested data. Container types do not need to match if nested.\nJAX provides the tree_util package for working with pytrees.\n\nfrom jax import tree_util\n\ntree = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\nprint('tree:', tree)\n\ntree: [1, {'k1': 2, 'k2': (3, 4)}, 5]\ntree_util.tree_map(lambda x: x*2, tree): [2, {'k1': 4, 'k2': (6, 8)}, 10]\n\n\nThe tree_map function is frequently used for updating a tree and its leaves.\n\ntree_util.tree_map(lambda x: x*2, tree)\n\n[2, {'k1': 4, 'k2': (6, 8)}, 10]\n\n\nWe can also provide a tuple of aditional trees of the same shape to the original tree to enable a function to operate on each leaf.\n\ntransformed_tree = tree_util.tree_map(lambda x: x*2, tree)\ntree_util.tree_map(lambda x,y: x+y, tree, transformed_tree)\n\n[3, {'k1': 6, 'k2': (9, 12)}, 15]\n\n\n\n# Linear feed-forward.\ndef predict(W, b, x):\n  return jnp.dot(x, W) + b\n\n# Loss function: Mean squared error.\ndef mse(W, b, x_batched, y_batched):\n  # Define the squared loss for a single pair (x,y)\n  def squared_error(x, y):\n    y_pred = predict(W, b, x)\n    return jnp.inner(y-y_pred, y-y_pred) / 2.0\n  # We vectorize the previous to compute the average of the loss on all samples.\n  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n\n\n# Set problem dimensions.\nn_samples = 20\nx_dim = 10\ny_dim = 5\n\n# Generate random ground truth W and b.\nkey = random.PRNGKey(0)\nk1, k2 = random.split(key)\nW = random.normal(k1, (x_dim, y_dim))\nb = random.normal(k2, (y_dim,))\n\n# Generate samples with additional noise.\nkey_sample, key_noise = random.split(k1)\nx_samples = random.normal(key_sample, (n_samples, x_dim))\ny_samples = predict(W, b, x_samples) + 0.1 * random.normal(key_noise,(n_samples, y_dim))\nprint('x shape:', x_samples.shape, '; y shape:', y_samples.shape)\n\nx shape: (20, 10) ; y shape: (20, 5)\n\n\nIn this linear regression, params is a pytree which contains W and b.\n\n# Linear feed-forward that takes a params pytree.\ndef predict_pytree(params, x):\n  return jnp.dot(x, params['W']) + params['b']\n\n# Loss function: Mean squared error.\ndef mse_pytree(params, x_batched,y_batched):\n  # Define the squared loss for a single pair (x,y)\n  def squared_error(x,y):\n    y_pred = predict_pytree(params, x)\n    return jnp.inner(y-y_pred, y-y_pred) / 2.0\n  # We vectorize the previous to compute the average of the loss on all samples.\n  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n\n# Initialize estimated W and b with zeros. Store in a pytree.\nparams = {'W': jnp.zeros_like(W), 'b': jnp.zeros_like(b)}\n\nJAX can differentiate the pytree parameters\n\njax.grad(mse_pytree)(params, x_samples, y_samples)\n\n{'W': Array([[ 3.02512199e-05,  2.38317996e-04,  5.86672686e-05,\n          1.45167112e-04, -1.08840875e-04],\n        [-7.21593387e-05, -5.92094846e-04, -1.44919526e-04,\n         -3.55741940e-04,  2.12500338e-04],\n        [ 6.48805872e-06,  5.42663038e-05,  1.30501576e-05,\n          3.26364461e-05, -1.73687004e-05],\n        [-1.58965122e-05, -1.67803839e-04, -3.67928296e-05,\n         -9.91356210e-05,  1.37500465e-05],\n        [-9.83832870e-05, -7.85302604e-04, -1.94110908e-04,\n         -4.74306522e-04,  3.20815481e-04],\n        [-6.23832457e-05, -5.39575703e-04, -1.28836837e-04,\n         -3.22562526e-04,  1.54131034e-04],\n        [-8.21873546e-05, -6.98693097e-04, -1.67359249e-04,\n         -4.20103315e-04,  2.30040867e-04],\n        [-7.44033605e-06, -5.03805932e-05, -1.21158082e-05,\n         -3.23755667e-05,  4.07989137e-05],\n        [ 2.81375833e-06,  5.13494015e-05,  9.00402665e-06,\n          2.99257226e-05,  1.63719524e-05],\n        [-2.90344469e-05, -2.14974396e-04, -5.49759716e-05,\n         -1.30489469e-04,  1.08879060e-04]], dtype=float32),\n 'b': Array([-3.0185096e-05, -2.2265501e-04, -5.7548052e-05, -1.3502731e-04,\n         1.1305924e-04], dtype=float32)}\n\n\n\n@jax.jit\ndef update_params_pytree(params, learning_rate, x_samples, y_samples):\n  params = jax.tree_util.tree_map(\n        lambda p, g: p - learning_rate * g, params,\n        jax.grad(mse_pytree)(params, x_samples, y_samples))\n  return params\n\nlearning_rate = 0.3  # Gradient step size.\nprint('Loss for \"true\" W,b: ', mse_pytree({'W': W, 'b': b}, x_samples, y_samples))\nfor i in range(101):\n  # Perform one gradient update.\n  params = update_params_pytree(params, learning_rate, x_samples, y_samples)\n  if (i % 5 == 0):\n    print(f\"Loss step {i}: \", mse_pytree(params, x_samples, y_samples))\n\nLoss for \"true\" W,b:  0.02363979\nLoss step 0:  10.97141\nLoss step 5:  1.0798324\nLoss step 10:  0.3795825\nLoss step 15:  0.17855297\nLoss step 20:  0.094415195\nLoss step 25:  0.054522194\nLoss step 30:  0.03448924\nLoss step 35:  0.024058029\nLoss step 40:  0.018480862\nLoss step 45:  0.015438682\nLoss step 50:  0.01375394\nLoss step 55:  0.0128103\nLoss step 60:  0.012277315\nLoss step 65:  0.011974388\nLoss step 70:  0.011801446\nLoss step 75:  0.011702419\nLoss step 80:  0.011645543\nLoss step 85:  0.011612838\nLoss step 90:  0.011594015\nLoss step 95:  0.011583163\nLoss step 100:  0.011576912\n\n\nHere we can also use jax.value_and_grad() to compute both the return value of the input function and its gradient.\n\n# Using jax.value_and_grad instead:\nloss_grad_fn = jax.value_and_grad(mse_pytree)\nfor i in range(101):\n  # Note that here the loss is computed before the param update.\n    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n    params = jax.tree_util.tree_map(\n        lambda p, g: p - learning_rate * g, params, grads)\n    if (i % 5 == 0):\n        print(f\"Loss step {i}: \", loss_val)\n\nLoss step 0:  0.011576912\nLoss step 5:  0.011573299\nLoss step 10:  0.011571216\nLoss step 15:  0.011570027\nLoss step 20:  0.0115693165\nLoss step 25:  0.011568918\nLoss step 30:  0.011568695\nLoss step 35:  0.01156855\nLoss step 40:  0.011568478\nLoss step 45:  0.011568436\nLoss step 50:  0.011568408\nLoss step 55:  0.011568391\nLoss step 60:  0.01156838\nLoss step 65:  0.011568381\nLoss step 70:  0.01156838\nLoss step 75:  0.011568385\nLoss step 80:  0.011568374\nLoss step 85:  0.01156838\nLoss step 90:  0.01156837\nLoss step 95:  0.0115683675\nLoss step 100:  0.01156837"
  },
  {
    "objectID": "exercise1_jax_mnist.html",
    "href": "exercise1_jax_mnist.html",
    "title": "4  Exercise 1: MNIST in JAX",
    "section": "",
    "text": "Code mostly ported with thanks from DeepMind’s examples.\nUsing minimal dependencies and pure JAX functions, we train a simple neural network to classify MNIST digits.\nFirstly, some data loading functions. JAX can also leverage both TensorFlow and PyTorch data loading capabilites.\n\nimport array\nimport gzip\nimport os\nfrom os import path\nimport struct\nimport urllib.request\n\nimport numpy as np\n\n\n_DATA = \"/tmp/jax_example_data/\"\n\n\ndef _download(url, filename):\n  \"\"\"Download a url to a file in the JAX data temp directory.\"\"\"\n  if not path.exists(_DATA):\n    os.makedirs(_DATA)\n  out_file = path.join(_DATA, filename)\n  if not path.isfile(out_file):\n    urllib.request.urlretrieve(url, out_file)\n    print(f\"downloaded {url} to {_DATA}\")\n\n\ndef _partial_flatten(x):\n  \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n  return np.reshape(x, (x.shape[0], -1))\n\n\ndef _one_hot(x, k, dtype=np.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return np.array(x[:, None] == np.arange(k), dtype)\n\n\ndef mnist_raw():\n  \"\"\"Download and parse the raw MNIST dataset.\"\"\"\n  # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n  base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n\n  def parse_labels(filename):\n    with gzip.open(filename, \"rb\") as fh:\n      _ = struct.unpack(\"&gt;II\", fh.read(8))\n      return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n\n  def parse_images(filename):\n    with gzip.open(filename, \"rb\") as fh:\n      _, num_data, rows, cols = struct.unpack(\"&gt;IIII\", fh.read(16))\n      return np.array(array.array(\"B\", fh.read()),\n                      dtype=np.uint8).reshape(num_data, rows, cols)\n\n  for filename in [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n                   \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]:\n    _download(base_url + filename, filename)\n\n  train_images = parse_images(path.join(_DATA, \"train-images-idx3-ubyte.gz\"))\n  train_labels = parse_labels(path.join(_DATA, \"train-labels-idx1-ubyte.gz\"))\n  test_images = parse_images(path.join(_DATA, \"t10k-images-idx3-ubyte.gz\"))\n  test_labels = parse_labels(path.join(_DATA, \"t10k-labels-idx1-ubyte.gz\"))\n\n  return train_images, train_labels, test_images, test_labels\n\n\ndef mnist(permute_train=False):\n  \"\"\"Download, parse and process MNIST data to unit scale and one-hot labels.\"\"\"\n  train_images, train_labels, test_images, test_labels = mnist_raw()\n\n  train_images = _partial_flatten(train_images) / np.float32(255.)\n  test_images = _partial_flatten(test_images) / np.float32(255.)\n  train_labels = _one_hot(train_labels, 10)\n  test_labels = _one_hot(test_labels, 10)\n\n  if permute_train:\n    perm = np.random.RandomState(0).permutation(train_images.shape[0])\n    train_images = train_images[perm]\n    train_labels = train_labels[perm]\n\n  return train_images, train_labels, test_images, test_labels\n\n\n# Copyright 2018 The JAX Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nA basic MNIST example using Numpy and JAX.\n\"\"\"\n\nimport time\n\nimport numpy.random as npr\n\nfrom jax import jit, grad, random\nfrom jax.scipy.special import logsumexp\nimport jax.numpy as jnp\nfrom examples import datasets\n\n\n# TODO: If you could only @jit one of these functions,\n# which would be the best candidate?\n# Add the @jit decorator to the function of your choice.\n\ndef init_random_params(scale, layer_sizes):\n    # TODO Initialize a random PRNGKey\n    key = pass\n    # TODO Split the PRNGKey into two new keys\n    key1, key2 = pass\n    params = [(scale * random.normal(key1, (m, n)), scale * random.normal(key2, (n,)))\n              for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]\n    print(params)\n    return params\n\n\ndef predict(params, inputs):\n  activations = inputs\n  for w, b in params[:-1]:\n    outputs = jnp.dot(activations, w) + b\n    activations = jnp.tanh(outputs)\n\n  final_w, final_b = params[-1]\n  logits = jnp.dot(activations, final_w) + final_b\n  return logits - logsumexp(logits, axis=1, keepdims=True)\n\ndef loss(params, batch):\n  inputs, targets = batch\n  preds = predict(params, inputs)\n  return -jnp.mean(jnp.sum(preds * targets, axis=1))\n\ndef accuracy(params, batch):\n  inputs, targets = batch\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\n\nif __name__ == \"__main__\":\n  layer_sizes = [784, 1024, 1024, 10]\n  param_scale = 0.1\n  step_size = 0.001\n  num_epochs = 10\n  batch_size = 128\n\n  train_images, train_labels, test_images, test_labels = mnist()\n  num_train = train_images.shape[0]\n  num_complete_batches, leftover = divmod(num_train, batch_size)\n  num_batches = num_complete_batches + bool(leftover)\n\n  def data_stream():\n    rng = npr.RandomState(0)\n    while True:\n      perm = rng.permutation(num_train)\n      for i in range(num_batches):\n        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n        yield train_images[batch_idx], train_labels[batch_idx]\n  batches = data_stream()\n\n  def update(params, batch):\n    # TODO: use JAX's transformations to\n    # find the gradients of the loss function w.r.t. params, batch\n    # Replace `pass` with your code\n    grads = pass\n    return [(w - step_size * dw, b - step_size * db)\n            for (w, b), (dw, db) in zip(params, grads)]\n\n  params = init_random_params(param_scale, layer_sizes)\n  for epoch in range(num_epochs):\n    start_time = time.time()\n    for _ in range(num_batches):\n      params = update(params, next(batches))\n    epoch_time = time.time() - start_time\n\n    train_acc = accuracy(params, (train_images, train_labels))\n    test_acc = accuracy(params, (test_images, test_labels))\n    print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n    print(f\"Training set accuracy {train_acc}\")\n    print(f\"Test set accuracy {test_acc}\")"
  },
  {
    "objectID": "flax_foundations.html",
    "href": "flax_foundations.html",
    "title": "5  Flax Foundations",
    "section": "",
    "text": "Efficient and flexible model development\nBy combining JAX’s auto-differentiation and Flax’s modular design, developers can easily construct and train state-of-the-art deep learning models. JAX/Flax traces pure functions and compiles for GPU and TPU accelerators.\n\nimport jax\nfrom typing import Any, Callable, Sequence\nfrom jax import lax, random, numpy as jnp\nfrom flax.core import freeze, unfreeze\nfrom flax import linen as nn\n\n\n# Here's a single dense layer that takes a number of features as input\nmodel = nn.Dense(features=5)\n\nkey1, key2 = random.split(random.PRNGKey(0))\n# Dummy input data\nx = random.normal(key1, (10,))\n# Initialize the model\nparams = model.init(key2, x)\n# Forward pass\nmodel.apply(params, x)\n\nWARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nArray([-1.3721193 ,  0.61131495,  0.6442836 ,  2.2192965 , -1.1271116 ],      dtype=float32)\n\n\nNote we only mention to Flax the number of features for the output of the model, rather than specifying the size of the input. Flax works out the correct kernel size for us!\nLet’s take a look at the pytree:\n\n# Check output shapes\njax.tree_util.tree_map(lambda x: x.shape, params)\n\nFrozenDict({\n    params: {\n        bias: (5,),\n        kernel: (10, 5),\n    },\n})\n\n\nNotice the parameters are stored in a FrozenDict, which prevents any mutation of the values.\n\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\n\n# Dummy data\ninputs = jnp.array([[0.2, 0.3, 0.4], [0.1, 0.2, 0.3]])\ntargets = jnp.array([[0.5], [0.8]])\n\n# Simple feedforward neural network\nclass SimpleNetwork(nn.Module):\n    hidden_size: int\n    output_size: int\n\n    def setup(self):\n        self.dense1 = nn.Dense(self.hidden_size)\n        self.dense2 = nn.Dense(self.output_size)\n\n    def __call__(self, x):\n        x = self.dense1(x)\n        x = nn.relu(x)\n        x = self.dense2(x)\n        return x\n\n# Initialization\nhidden_size = 16\noutput_size = 1\nrng = jax.random.PRNGKey(0)\nmodel = SimpleNetwork(hidden_size, output_size)\nparams = model.init(rng, inputs)\ntree = jax.tree_util.tree_map(lambda inputs: inputs.shape, params) # Checking output shapes\nprint(tree)\n\n# Forward pass\npredictions = model.apply(params, inputs)\n\nprint(f\"Inputs: \\n{inputs}\")\nprint(f\"\\nPredictions: \\n{predictions}\")\nprint(f\"\\nTarget data: \\n{targets}\")\n\nFrozenDict({\n    params: {\n        dense1: {\n            bias: (16,),\n            kernel: (3, 16),\n        },\n        dense2: {\n            bias: (1,),\n            kernel: (16, 1),\n        },\n    },\n})\nInputs: \n[[0.2 0.3 0.4]\n [0.1 0.2 0.3]]\n\nPredictions: \n[[-0.01026188]\n [-0.01458298]]\n\nTarget data: \n[[0.5]\n [0.8]]\n\n\nIn this example, we defined our model explicitly using setup. We can also define architecrures using nn.compact, which allows us to define a modulea s a single method. This can lead to cleaner code if you are writing custom layers.\nHere’s our SimpleNetwork again, using setup.\n\nclass SimpleNetwork(nn.Module):\n    hidden_size: int\n    output_size: int\n\n    def setup(self):\n        self.dense1 = nn.Dense(self.hidden_size)\n        self.dense2 = nn.Dense(self.output_size)\n\n    def __call__(self, x):\n        x = self.dense1(x)\n        x = nn.relu(x)\n        x = self.dense2(x)\n        return x\n\nAnd using nn.compact:\n\nclass SimpleNetwork(nn.Module):\n  hidden_size: int\n  output_size: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(hidden_size, name=\"dense1\")(x)\n    x = nn.relu(x)\n    x = nn.Dense(output_size, name=\"dense2\")(x)\n    return x\n\nIf you are porting models from PyTorch, or prefer explicit definition and separation of submodules, setup may suit. nn.compact may be best for reducing duplication, writing code that looks closer to mathematical notation, or if you are using shape inference (parameters dependant on shapes of inputs unknown at initialization).\n\n5.0.1 Flax modules\nFlax it easy to incorporate training techniques such as batch normalization and learning rate scheduling via the flax.linen.Module.\nHere’s our simple multi-layer perceptron again:\n\nclass SimpleNetwork(nn.Module):\n  hidden_size: int\n  output_size: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(hidden_size, name=\"dense1\")(x)\n    x = nn.relu(x)\n    x = nn.Dense(output_size, name=\"dense2\")(x)\n    return x\n\nBatch normalization is a regularization technique which computes running averages over feature dimensions. This speeds up training cycles and improves convergence. To apply batch normalization, we call upon flax.linen.BatchNorm.\n\nclass SimpleNetwork(nn.Module):\n  hidden_size: int\n  output_size: int\n\n  @nn.compact\n  def __call__(self, x, train: bool):\n    x = nn.Dense(hidden_size, name=\"dense1\")(x)\n    x = nn.BatchNorn(use_running_average=not train)(x)\n    x = nn.relu(x)\n    x = nn.Dense(output_size, name=\"dense2\")(x)\n    return x\n\n\n\n5.0.2 Dropout\nDropout is another (stochastic) regularization technique that randomly removes units in a network to improve reduce overfitting and improve generalization.\nDropout requires our PRNG skills to endure it is a random operation.\nWhen splitting a key, we can simply split into three keys, granting the third for flax.linen.dropout.\n\nkey = jax.random.PRNGKey(seed=0)\nmain_key, params_key, dropout_key = jax.random.split(key=key, num=3)\n\nThen add the module to our model:\n\nclass SimpleNetwork(nn.Module):\n  hidden_size: int\n  output_size: int\n\n  @nn.compact\n  def __call__(self, x, train: bool):\n    x = nn.Dense(hidden_size, name=\"dense1\")(x)\n    x = nn.Dropout(rate=0.5, deterministic=not train)(x)\n    x = nn.BatchNorm(use_running_average=not train)(x)\n    x = nn.relu(x)\n    x = nn.Dense(output_size, name=\"dense2\")(x)\n    return x\n\nWe can then initialize the model:\n\nsimple_net = SimpleNetwork(hidden_size=5, output_size=1)\nx = jnp.empty((3, 4, 4, 5, 5))\n# Dropout is enabled via `deterministic=True`.\nvariables = simple_net.init(params_key, x, train=False)\nparams = variables['params']\n\n\n\n5.0.3 Train states\nA “train state” is the mutable state of a model during training, including properties such as its parameters (weights) and optimizer state.\nThe train state is typically represented as an instance of the flax.training.TrainState class, which encapsulates and provides methods to update the state.\nOne of the features of JAX/Flax is its functional programming characteristic of immutability. Models are updates are purely functional, enabling model parallelism and efficient training.\n\n# Example, will not run\n\ndef create_train_state(rng, learning_rate, momentum):\n  \"\"\"Creates initial `TrainState`.\"\"\"\n  cnn = CNN()\n  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n  tx = optax.sgd(learning_rate, momentum)\n  return train_state.TrainState.create(\n      apply_fn=cnn.apply, params=params, tx=tx)\n\n\n\n\n5.0.4 Optax\nOptax is a gradient processing and optimization package. It is generally used with Flax as follows:\nCreate an optimizer state from parameters using any optimization method (eg optax.rmsprop). Compute loss gradients using value_and_grad(). Call the Optax update function to update the internal optimizer state to work out how to tweak the parameters. Use apply_updates to apply update the to the parameters.\nFor example (will not run):\n\nimport optax\n\noptimizer = optax.adam(learning_rate=learning_rate)\noptimizer_state = optimizer.init(params)\nloss_grad_func = jax.value_and_grad(mse)\n\nfor i in range(10):\n  loss, grads = loss_grad_func(params, x_samples, y_samples)\n  updates, optimizer_state = optimizer.update(grads, optimizer_state)\n  params = optax.apply_updates(params, updates)\n  if i % 10 == 0:\n    print('Loss step {}: '.format(i), loss)\n\nMNIST Example\n\nfrom absl import logging\nfrom flax import linen as nn\nfrom flax.metrics import tensorboard\nfrom flax.training import train_state\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\nimport tensorflow_datasets as tfds\n\n\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    return x\n\n\n@jax.jit\ndef apply_model(state, images, labels):\n  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n  def loss_fn(params):\n    logits = state.apply_fn({'params': params}, images)\n    one_hot = jax.nn.one_hot(labels, 10)\n    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n    return loss, logits\n\n  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n  (loss, logits), grads = grad_fn(state.params)\n  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n  return grads, loss, accuracy\n\n\n@jax.jit\ndef update_model(state, grads):\n  return state.apply_gradients(grads=grads)\n\n\ndef train_epoch(state, train_ds, batch_size, rng):\n  \"\"\"Train for a single epoch.\"\"\"\n  train_ds_size = len(train_ds['image'])\n  steps_per_epoch = train_ds_size // batch_size\n\n  perms = jax.random.permutation(rng, len(train_ds['image']))\n  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n  perms = perms.reshape((steps_per_epoch, batch_size))\n\n  epoch_loss = []\n  epoch_accuracy = []\n\n  for perm in perms:\n    batch_images = train_ds['image'][perm, ...]\n    batch_labels = train_ds['label'][perm, ...]\n    grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n    state = update_model(state, grads)\n    epoch_loss.append(loss)\n    epoch_accuracy.append(accuracy)\n  train_loss = np.mean(epoch_loss)\n  train_accuracy = np.mean(epoch_accuracy)\n  return state, train_loss, train_accuracy\n\n\ndef get_datasets():\n  \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n  ds_builder = tfds.builder('mnist')\n  ds_builder.download_and_prepare()\n  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n  train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n  test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n  return train_ds, test_ds\n\n\ndef create_train_state(rng, learning_rate, momentum):\n  \"\"\"Creates initial `TrainState`.\"\"\"\n  cnn = CNN()\n  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n  tx = optax.sgd(learning_rate, momentum)\n  return train_state.TrainState.create(\n      apply_fn=cnn.apply, params=params, tx=tx)\n\n\ndef train_and_evaluate(learning_rate, momentum,\n                       batch_size, num_epochs) -&gt; train_state.TrainState:\n  \"\"\"Execute model training and evaluation loop.\n\n  Args:\n    config: Hyperparameter configuration for training and evaluation.\n    workdir: Directory where the tensorboard summaries are written to.\n\n  Returns:\n    The train state (which includes the `.params`).\n  \"\"\"\n  train_ds, test_ds = get_datasets()\n  rng = jax.random.PRNGKey(0)\n\n  rng, init_rng = jax.random.split(rng)\n  state = create_train_state(init_rng, 0.01, 0.9 )\n\n  for epoch in range(1, num_epochs + 1):\n    rng, input_rng = jax.random.split(rng)\n    state, train_loss, train_accuracy = train_epoch(state, train_ds,\n                                                    64,\n                                                    input_rng)\n    _, test_loss, test_accuracy = apply_model(state, test_ds['image'],\n                                              test_ds['label'])\n\n    print(\n        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n        % (epoch, train_loss, train_accuracy * 100, test_loss,\n           test_accuracy * 100))\n\n    print('train_loss', train_loss, epoch)\n    print('train_accuracy', train_accuracy, epoch)\n    print('test_loss', test_loss, epoch)\n    print('test_accuracy', test_accuracy, epoch)\n\n  return state\n\n\ntrain_and_evaluate(0.01, 0.9, 128, 1)"
  },
  {
    "objectID": "exercise2_linear_reg_flax.html",
    "href": "exercise2_linear_reg_flax.html",
    "title": "6  Exercise 2: Linear Regression in Flax",
    "section": "",
    "text": "import jax\nfrom jax import numpy as jnp, random, lax, jit\nfrom flax import linen as nn\n\n# TODO: Data preparation\n# Create variables X and Y:\n# X is a 1 x 10 matrix\n# Y is a 1-dimensional array of size 5\n# Some references on generating matrices here:\n# https://flax.readthedocs.io/en/latest/guides/jax_for_the_impatient.html\n\nX = pass\nY = pass\n\n# TODO: create a model of one Dense layer with 5 features \n# For help:\n# https://flax.readthedocs.io/en/latest/guides/flax_basics.html\nmodel = nn.Dense(features=5)\n\n@jit\ndef predict(params):\n  return model.apply({'params': params}, X)\n\n@jit\ndef loss_fn(params):\n  return jnp.mean(jnp.abs(Y - predict(params)))\n\n# Initialize the model with random values\n# use random number generator ('rng') as input\n# to initialize params based on input shape of 'X'.\n@jit\ndef init_params(rng):\n  mlp_variables = model.init({'params': rng}, X)\n  return mlp_variables['params']\n\n# TODO\n# use the init_params function and \n# jax.random to initalize random params\n# using PRNGKey\nparams = None\nprint(\"initial params\", params)\n\n# Run SGD.\nfor i in range(50):\n  # TODO use jax transformations to extract the loss value\n  # and gradients of the loss with respect to the params\n  loss, grad = pass\n  print(i, \"loss = \", loss, \"Yhat = \", predict(params))\n  lr = 0.03\n  params = jax.tree_util.tree_map(lambda x, d: x - lr * d, params, grad)"
  },
  {
    "objectID": "Resnet.html",
    "href": "Resnet.html",
    "title": "7  Let’s build a ResNet!",
    "section": "",
    "text": "Now that we have an understanding of JAX, and how to build neural network layers and optimizations in Flax, let’s put our skills to implementing one of the most cited academic publications in machine learning: Deep Residual Learning for Image Recognition.\n\n7.0.1 The idea\nThe paper authors found that after batchnorm, networks with more layers often performed worse that those with fewer.\n (Image from paper linked above by Kaimimg He and others).\nThe researchers experimented with the idea of adding extra layers as an ‘identity mapping’, which means they have parameters and are trainable, but which return inputs without changing them.\nThis idea resulted in ‘skip connections’, which leap over convolutions as in this diagram:\n\n\n\nSkip connections\n\n\nThe skip connections make the network make the larger architecture easier to train, and prevent overfitting.\n\n7.0.1.1 Residuals\nA ‘residual’ is basically a prediction minus the target. ResNet blocks beat most earlier benchmarks because rather than asking them to predict the target, they predict the difference between the target and the prediction. This architecture proved very strong in detecting slight differences in images (is it a wolf or an off-leash Husky running through a dark forest?).\n\nfrom functools import partial\n\nfrom flax import linen as nn\nimport jax.numpy as jnp\nfrom typing import Any, Callable, Sequence, Tuple\n\n\nclass ResNetBlock(nn.Module):\n  filters: int\n  strides: Tuple[int, int] = (1, 1)\n\n  @nn.compact\n  def __call__(self, x):\n    residual = x\n    y = nn.Conv(self.filters, (3, 3), self.strides)(x)\n    y = nn.BatchNorm()(y)\n    y = nn.relu(y)\n    y = nn.Conv(self.filters, (3, 3))(y)\n    return x + y\n\n    if residual.shape != y.shape:\n          residual = nn.conv(self.filters, (1, 1),\n                              self.strides, name='conv_proj')(residual)\n          residual = self.norm(name='norm_proj')(residual)\n\n    return self.act(residual + y)\n\n\n\n\n\n7.0.2 Bottleneck layers\nTo enable training deeper models without spiking memory and computation use, we can use bottleneck layers. These were also introduced in the original paper as suitable for ResNets with a depth of 50 or more layers.\nIn our original ResNet layer, we have two convolutions with kernel size 3. Bottleneck layers use a 1 x 1 convolution at the start and end, and a 3 x 3 layer in between.\n\n\n\nBottleneck layers\n\n\nThese improve training when used with deeper models since they allow us to add more filters. Filters mean we can reduce the color channels of images, then restore them – hence their name.\n\nclass BottleneckResNetBlock(nn.Module):\n  filters: int\n  strides: Tuple[int, int] = (1, 1)\n\n  @nn.compact\n  def __call__(self, x):\n    residual = x\n    y = nn.Conv(self.filters, (1, 1))(x)\n    y = nn.BatchNorm()(y)\n    y = nn.relu(y)\n    y = nn.Conv(self.filters, (3, 3), self.strides)(y)\n    y = nn.BatchNorm()(y)\n    y = nn.relu(y)\n    y = nn.Conv(self.filters * 4, (1, 1), self.strides)(y)\n    y = nn.BatchNorm(scale_init=nn.initializers.zeros_init())(y)\n\n    if residual.shape != y.shape:\n          residual = nn.Conv(self.filters * 4, (1, 1),\n                              strides=(1, 1), name='conv_proj')(residual)\n          residual = self.BatchNorm(name='norm_proj')(residual)\n\n    return nn.relu(residual + y)\n\n\n\n\n\n7.0.3 Creating the ResNet\nNow we have our blocks, we can stack them together to create a fully-fledged ResNet.\nBlocks are generally grouped by shape, so if our model has [2,2,2,2] blocks, it means we have four groups of 2 blocks.\nWe will use the original ResNetBlock for those &lt; 50 layers, and the BottleneckResNetBlock for larger architectures.\nThe various ResNet sizes (ResNet18, ResNet50 etc) simply denote the number of layers.\nA ResNet 18’s blocks are [2,2,2,2], so how do we get to 18 layers?\nWe have 1 initial conv layer, 8 conv layers in residual blocks, a final conv layer, which gives us 10 layers. The remaining 8 are fully connected layers that follow the last conv layer, which typically serve as the classifier head and output predictions.\n\nfrom functools import partial\n\nclass ResNet(nn.Module):\n  num_classes: int\n  block_class: nn.Module\n  num_blocks: Sequence[int]\n  filters: int = 64\n  dtype: Any = jnp.float32\n\n\n  @nn.compact\n  def __call__(self, x, train: bool = True):\n    x = nn.Conv(self.filters, (7, 7), (2, 2),\n             padding=[(3, 3), (3, 3)],\n             use_bias=False,\n             name='conv_init')(x)\n    x = nn.BatchNorm(name='bn_init')(x, use_running_average=not train)\n    x = nn.relu(x)\n    x = nn.max_pool(x, (3, 3), (2, 2), padding='SAME')\n\n    for i, block_size in enumerate(self.num_blocks):\n      for j in range(block_size):\n        strides = (2, 2) if i &gt; 0 and j == 0 else (1, 1)\n        x = self.block_class(self.filters * (2**i),\n                             strides=strides)(x)\n\n    x = jnp.mean(x, axis=(1, 2))\n    x = nn.Dense(self.num_classes, dtype=self.dtype)(x)\n    x = jnp.asarray(x, self.dtype)\n\n    return x\n\nResNet18 = partial(ResNet, num_blocks=[2, 2, 2, 2],\n                   block_class=ResNetBlock)\nResNet34 = partial(ResNet, num_blocks=[3, 4, 6, 3],\n                   block_class=ResNetBlock)\nResNet50 = partial(ResNet, num_blocks=[3, 4, 6, 3],\n                   block_class=BottleneckResNetBlock)\nResNet101 = partial(ResNet, num_blocks=[3, 4, 23, 3],\n                    block_class=BottleneckResNetBlock)\nResNet152 = partial(ResNet, num_blocks=[3, 8, 36, 3],\n                    block_class=BottleneckResNetBlock)\nResNet200 = partial(ResNet, num_blocks=[3, 24, 36, 3],\n                    block_class=BottleneckResNetBlock)"
  },
  {
    "objectID": "diffusion.html",
    "href": "diffusion.html",
    "title": "8  Diffusion for the curious",
    "section": "",
    "text": "TODO: Add speaker notes\n\n8.0.0.1 How do we learn the probabilities of what an image represents?\n\n\n\n8.0.0.2 Adding and detecting noise\n\n\n\n8.0.0.3 Enter the Unet\n\n\n\n8.0.0.4 Embeddings\n\n\n\n8.0.0.5 Latents\n\n\n\n8.0.0.6 Multi-modal\n\n\n\n8.0.0.7 Altogether"
  },
  {
    "objectID": "stable_diffusion.html#setup",
    "href": "stable_diffusion.html#setup",
    "title": "9  Stable Diffusion in JAX / Flax !",
    "section": "9.1 Setup",
    "text": "9.1 Setup\n\n!pip install flax transformers ftfy\n!pip install diffusers==0.9.0\n\n\nimport jax\n\n\nnum_devices = jax.device_count()\ndevice_type = jax.devices()[0].device_kind\n\nprint(f\"Found {num_devices} JAX devices of type {device_type}.\")\nassert \"TPU\" in device_type, \"Available device is not a TPU, please select TPU from Edit &gt; Notebook settings &gt; Hardware accelerator\"\n\nFound 8 JAX devices of type Cloud TPU.\n\n\nThen we import all the dependencies.\n\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\n\nfrom pathlib import Path\nfrom jax import pmap\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\nfrom PIL import Image\n\nfrom huggingface_hub import notebook_login\nfrom diffusers import FlaxStableDiffusionPipeline"
  },
  {
    "objectID": "stable_diffusion.html#model-loading",
    "href": "stable_diffusion.html#model-loading",
    "title": "9  Stable Diffusion in JAX / Flax !",
    "section": "9.2 Model Loading",
    "text": "9.2 Model Loading\nBefore using the model, you need to accept the model license in order to download and use the weights.\nThe license is designed to mitigate the potential harmful effects of such a powerful machine learning system. We request users to read the license entirely and carefully. Here we offer a summary:\n\nYou can’t use the model to deliberately produce nor share illegal or harmful outputs or content,\nWe claim no rights on the outputs you generate, you are free to use them and are accountable for their use which should not go against the provisions set in the license, and\nYou may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users.\n\nFlax weights are available in Hugging Face Hub as part of the Stable Diffusion repo. To use them, you need to be a registered user in Hugging Face Hub and use an access token for the code to work. You have two options to provide your access token:\n\nUse the huggingface-cli login command-line tool in your terminal and paste your token when prompted. It will be saved in a file in your computer.\nOr use notebook_login() in a notebook, which does the same thing.\n\nThe following cell will present a login interface unless you’ve already authenticated before in this computer. You’ll need to paste your access token.\n\nif not (Path.home()/'.huggingface'/'token').exists(): notebook_login()\n\nLogin successful\nYour token has been saved to /root/.huggingface/token\n\n\nTPU devices support bfloat16, an efficient half-float type. We’ll use it for our tests, but you can also use float32 to use full precision instead.\n\ndtype = jnp.bfloat16\n\nFlax is a functional framework, so models are stateless and parameters are stored outside them. Loading the pre-trained Flax pipeline will return both the pipeline itself and the model weights (or parameters). We are using a bf16 version of the weights, which leads to type warnings that you can safely ignore.\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision=\"bf16\",\n    dtype=dtype,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome of the weights of FlaxStableDiffusionSafetyChecker were initialized in bfloat16 precision from the model checkpoint at /root/.cache/huggingface/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/295cccdedbd5f87458186972858dc85c7e70c10a/safety_checker:\n[('concept_embeds',), ('concept_embeds_weights',), ('special_care_embeds',), ('special_care_embeds_weights',), ('vision_model', 'vision_model', 'embeddings', 'class_embedding'), ('vision_model', 'vision_model', 'embeddings', 'patch_embedding', 'kernel'), ('vision_model', 'vision_model', 'embeddings', 'position_embedding', 'embedding'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '12', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '13', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '14', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '15', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '16', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '17', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '18', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '19', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '20', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '21', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '22', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '23', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'layer_norm1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'layer_norm1', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'layer_norm2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'layer_norm2', 'scale'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'mlp', 'fc1', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'mlp', 'fc1', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'mlp', 'fc2', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'mlp', 'fc2', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'vision_model', 'encoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'vision_model', 'post_layernorm', 'bias'), ('vision_model', 'vision_model', 'post_layernorm', 'scale'), ('vision_model', 'vision_model', 'pre_layrnorm', 'bias'), ('vision_model', 'vision_model', 'pre_layrnorm', 'scale'), ('visual_projection', 'kernel')]\nYou should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\nSome of the weights of FlaxCLIPTextModel were initialized in bfloat16 precision from the model checkpoint at /root/.cache/huggingface/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/295cccdedbd5f87458186972858dc85c7e70c10a/text_encoder:\n[('text_model', 'embeddings', 'position_embedding', 'embedding'), ('text_model', 'embeddings', 'token_embedding', 'embedding'), ('text_model', 'encoder', 'layers', '0', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '0', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '0', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '0', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '0', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '0', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '0', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '0', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '1', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '1', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '1', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '1', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '1', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '1', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '1', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '1', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '10', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '10', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '10', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '10', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '10', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '10', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '10', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '10', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '11', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '11', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '11', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '11', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '11', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '11', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '11', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '11', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '2', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '2', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '2', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '2', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '2', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '2', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '2', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '2', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '3', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '3', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '3', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '3', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '3', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '3', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '3', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '3', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '4', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '4', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '4', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '4', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '4', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '4', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '4', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '4', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '5', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '5', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '5', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '5', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '5', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '5', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '5', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '5', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '6', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '6', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '6', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '6', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '6', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '6', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '6', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '6', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '7', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '7', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '7', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '7', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '7', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '7', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '7', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '7', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '8', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '8', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '8', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '8', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '8', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '8', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '8', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '8', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'encoder', 'layers', '9', 'layer_norm1', 'bias'), ('text_model', 'encoder', 'layers', '9', 'layer_norm1', 'scale'), ('text_model', 'encoder', 'layers', '9', 'layer_norm2', 'bias'), ('text_model', 'encoder', 'layers', '9', 'layer_norm2', 'scale'), ('text_model', 'encoder', 'layers', '9', 'mlp', 'fc1', 'bias'), ('text_model', 'encoder', 'layers', '9', 'mlp', 'fc1', 'kernel'), ('text_model', 'encoder', 'layers', '9', 'mlp', 'fc2', 'bias'), ('text_model', 'encoder', 'layers', '9', 'mlp', 'fc2', 'kernel'), ('text_model', 'encoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('text_model', 'encoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('text_model', 'encoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('text_model', 'encoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('text_model', 'encoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('text_model', 'encoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('text_model', 'encoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('text_model', 'encoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('text_model', 'final_layer_norm', 'bias'), ('text_model', 'final_layer_norm', 'scale')]\nYou should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\nSome of the weights of FlaxAutoencoderKL were initialized in bfloat16 precision from the model checkpoint at /root/.cache/huggingface/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/295cccdedbd5f87458186972858dc85c7e70c10a/vae:\n[('decoder', 'conv_in', 'bias'), ('decoder', 'conv_in', 'kernel'), ('decoder', 'conv_norm_out', 'bias'), ('decoder', 'conv_norm_out', 'scale'), ('decoder', 'conv_out', 'bias'), ('decoder', 'conv_out', 'kernel'), ('decoder', 'mid_block', 'attentions_0', 'group_norm', 'bias'), ('decoder', 'mid_block', 'attentions_0', 'group_norm', 'scale'), ('decoder', 'mid_block', 'attentions_0', 'key', 'bias'), ('decoder', 'mid_block', 'attentions_0', 'key', 'kernel'), ('decoder', 'mid_block', 'attentions_0', 'proj_attn', 'bias'), ('decoder', 'mid_block', 'attentions_0', 'proj_attn', 'kernel'), ('decoder', 'mid_block', 'attentions_0', 'query', 'bias'), ('decoder', 'mid_block', 'attentions_0', 'query', 'kernel'), ('decoder', 'mid_block', 'attentions_0', 'value', 'bias'), ('decoder', 'mid_block', 'attentions_0', 'value', 'kernel'), ('decoder', 'mid_block', 'resnets_0', 'conv1', 'bias'), ('decoder', 'mid_block', 'resnets_0', 'conv1', 'kernel'), ('decoder', 'mid_block', 'resnets_0', 'conv2', 'bias'), ('decoder', 'mid_block', 'resnets_0', 'conv2', 'kernel'), ('decoder', 'mid_block', 'resnets_0', 'norm1', 'bias'), ('decoder', 'mid_block', 'resnets_0', 'norm1', 'scale'), ('decoder', 'mid_block', 'resnets_0', 'norm2', 'bias'), ('decoder', 'mid_block', 'resnets_0', 'norm2', 'scale'), ('decoder', 'mid_block', 'resnets_1', 'conv1', 'bias'), ('decoder', 'mid_block', 'resnets_1', 'conv1', 'kernel'), ('decoder', 'mid_block', 'resnets_1', 'conv2', 'bias'), ('decoder', 'mid_block', 'resnets_1', 'conv2', 'kernel'), ('decoder', 'mid_block', 'resnets_1', 'norm1', 'bias'), ('decoder', 'mid_block', 'resnets_1', 'norm1', 'scale'), ('decoder', 'mid_block', 'resnets_1', 'norm2', 'bias'), ('decoder', 'mid_block', 'resnets_1', 'norm2', 'scale'), ('decoder', 'up_blocks_0', 'resnets_0', 'conv1', 'bias'), ('decoder', 'up_blocks_0', 'resnets_0', 'conv1', 'kernel'), ('decoder', 'up_blocks_0', 'resnets_0', 'conv2', 'bias'), ('decoder', 'up_blocks_0', 'resnets_0', 'conv2', 'kernel'), ('decoder', 'up_blocks_0', 'resnets_0', 'norm1', 'bias'), ('decoder', 'up_blocks_0', 'resnets_0', 'norm1', 'scale'), ('decoder', 'up_blocks_0', 'resnets_0', 'norm2', 'bias'), ('decoder', 'up_blocks_0', 'resnets_0', 'norm2', 'scale'), ('decoder', 'up_blocks_0', 'resnets_1', 'conv1', 'bias'), ('decoder', 'up_blocks_0', 'resnets_1', 'conv1', 'kernel'), ('decoder', 'up_blocks_0', 'resnets_1', 'conv2', 'bias'), ('decoder', 'up_blocks_0', 'resnets_1', 'conv2', 'kernel'), ('decoder', 'up_blocks_0', 'resnets_1', 'norm1', 'bias'), ('decoder', 'up_blocks_0', 'resnets_1', 'norm1', 'scale'), ('decoder', 'up_blocks_0', 'resnets_1', 'norm2', 'bias'), ('decoder', 'up_blocks_0', 'resnets_1', 'norm2', 'scale'), ('decoder', 'up_blocks_0', 'resnets_2', 'conv1', 'bias'), ('decoder', 'up_blocks_0', 'resnets_2', 'conv1', 'kernel'), ('decoder', 'up_blocks_0', 'resnets_2', 'conv2', 'bias'), ('decoder', 'up_blocks_0', 'resnets_2', 'conv2', 'kernel'), ('decoder', 'up_blocks_0', 'resnets_2', 'norm1', 'bias'), ('decoder', 'up_blocks_0', 'resnets_2', 'norm1', 'scale'), ('decoder', 'up_blocks_0', 'resnets_2', 'norm2', 'bias'), ('decoder', 'up_blocks_0', 'resnets_2', 'norm2', 'scale'), ('decoder', 'up_blocks_0', 'upsamplers_0', 'conv', 'bias'), ('decoder', 'up_blocks_0', 'upsamplers_0', 'conv', 'kernel'), ('decoder', 'up_blocks_1', 'resnets_0', 'conv1', 'bias'), ('decoder', 'up_blocks_1', 'resnets_0', 'conv1', 'kernel'), ('decoder', 'up_blocks_1', 'resnets_0', 'conv2', 'bias'), ('decoder', 'up_blocks_1', 'resnets_0', 'conv2', 'kernel'), ('decoder', 'up_blocks_1', 'resnets_0', 'norm1', 'bias'), ('decoder', 'up_blocks_1', 'resnets_0', 'norm1', 'scale'), ('decoder', 'up_blocks_1', 'resnets_0', 'norm2', 'bias'), ('decoder', 'up_blocks_1', 'resnets_0', 'norm2', 'scale'), ('decoder', 'up_blocks_1', 'resnets_1', 'conv1', 'bias'), ('decoder', 'up_blocks_1', 'resnets_1', 'conv1', 'kernel'), ('decoder', 'up_blocks_1', 'resnets_1', 'conv2', 'bias'), ('decoder', 'up_blocks_1', 'resnets_1', 'conv2', 'kernel'), ('decoder', 'up_blocks_1', 'resnets_1', 'norm1', 'bias'), ('decoder', 'up_blocks_1', 'resnets_1', 'norm1', 'scale'), ('decoder', 'up_blocks_1', 'resnets_1', 'norm2', 'bias'), ('decoder', 'up_blocks_1', 'resnets_1', 'norm2', 'scale'), ('decoder', 'up_blocks_1', 'resnets_2', 'conv1', 'bias'), ('decoder', 'up_blocks_1', 'resnets_2', 'conv1', 'kernel'), ('decoder', 'up_blocks_1', 'resnets_2', 'conv2', 'bias'), ('decoder', 'up_blocks_1', 'resnets_2', 'conv2', 'kernel'), ('decoder', 'up_blocks_1', 'resnets_2', 'norm1', 'bias'), ('decoder', 'up_blocks_1', 'resnets_2', 'norm1', 'scale'), ('decoder', 'up_blocks_1', 'resnets_2', 'norm2', 'bias'), ('decoder', 'up_blocks_1', 'resnets_2', 'norm2', 'scale'), ('decoder', 'up_blocks_1', 'upsamplers_0', 'conv', 'bias'), ('decoder', 'up_blocks_1', 'upsamplers_0', 'conv', 'kernel'), ('decoder', 'up_blocks_2', 'resnets_0', 'conv1', 'bias'), ('decoder', 'up_blocks_2', 'resnets_0', 'conv1', 'kernel'), ('decoder', 'up_blocks_2', 'resnets_0', 'conv2', 'bias'), ('decoder', 'up_blocks_2', 'resnets_0', 'conv2', 'kernel'), ('decoder', 'up_blocks_2', 'resnets_0', 'conv_shortcut', 'bias'), ('decoder', 'up_blocks_2', 'resnets_0', 'conv_shortcut', 'kernel'), ('decoder', 'up_blocks_2', 'resnets_0', 'norm1', 'bias'), ('decoder', 'up_blocks_2', 'resnets_0', 'norm1', 'scale'), ('decoder', 'up_blocks_2', 'resnets_0', 'norm2', 'bias'), ('decoder', 'up_blocks_2', 'resnets_0', 'norm2', 'scale'), ('decoder', 'up_blocks_2', 'resnets_1', 'conv1', 'bias'), ('decoder', 'up_blocks_2', 'resnets_1', 'conv1', 'kernel'), ('decoder', 'up_blocks_2', 'resnets_1', 'conv2', 'bias'), ('decoder', 'up_blocks_2', 'resnets_1', 'conv2', 'kernel'), ('decoder', 'up_blocks_2', 'resnets_1', 'norm1', 'bias'), ('decoder', 'up_blocks_2', 'resnets_1', 'norm1', 'scale'), ('decoder', 'up_blocks_2', 'resnets_1', 'norm2', 'bias'), ('decoder', 'up_blocks_2', 'resnets_1', 'norm2', 'scale'), ('decoder', 'up_blocks_2', 'resnets_2', 'conv1', 'bias'), ('decoder', 'up_blocks_2', 'resnets_2', 'conv1', 'kernel'), ('decoder', 'up_blocks_2', 'resnets_2', 'conv2', 'bias'), ('decoder', 'up_blocks_2', 'resnets_2', 'conv2', 'kernel'), ('decoder', 'up_blocks_2', 'resnets_2', 'norm1', 'bias'), ('decoder', 'up_blocks_2', 'resnets_2', 'norm1', 'scale'), ('decoder', 'up_blocks_2', 'resnets_2', 'norm2', 'bias'), ('decoder', 'up_blocks_2', 'resnets_2', 'norm2', 'scale'), ('decoder', 'up_blocks_2', 'upsamplers_0', 'conv', 'bias'), ('decoder', 'up_blocks_2', 'upsamplers_0', 'conv', 'kernel'), ('decoder', 'up_blocks_3', 'resnets_0', 'conv1', 'bias'), ('decoder', 'up_blocks_3', 'resnets_0', 'conv1', 'kernel'), ('decoder', 'up_blocks_3', 'resnets_0', 'conv2', 'bias'), ('decoder', 'up_blocks_3', 'resnets_0', 'conv2', 'kernel'), ('decoder', 'up_blocks_3', 'resnets_0', 'conv_shortcut', 'bias'), ('decoder', 'up_blocks_3', 'resnets_0', 'conv_shortcut', 'kernel'), ('decoder', 'up_blocks_3', 'resnets_0', 'norm1', 'bias'), ('decoder', 'up_blocks_3', 'resnets_0', 'norm1', 'scale'), ('decoder', 'up_blocks_3', 'resnets_0', 'norm2', 'bias'), ('decoder', 'up_blocks_3', 'resnets_0', 'norm2', 'scale'), ('decoder', 'up_blocks_3', 'resnets_1', 'conv1', 'bias'), ('decoder', 'up_blocks_3', 'resnets_1', 'conv1', 'kernel'), ('decoder', 'up_blocks_3', 'resnets_1', 'conv2', 'bias'), ('decoder', 'up_blocks_3', 'resnets_1', 'conv2', 'kernel'), ('decoder', 'up_blocks_3', 'resnets_1', 'norm1', 'bias'), ('decoder', 'up_blocks_3', 'resnets_1', 'norm1', 'scale'), ('decoder', 'up_blocks_3', 'resnets_1', 'norm2', 'bias'), ('decoder', 'up_blocks_3', 'resnets_1', 'norm2', 'scale'), ('decoder', 'up_blocks_3', 'resnets_2', 'conv1', 'bias'), ('decoder', 'up_blocks_3', 'resnets_2', 'conv1', 'kernel'), ('decoder', 'up_blocks_3', 'resnets_2', 'conv2', 'bias'), ('decoder', 'up_blocks_3', 'resnets_2', 'conv2', 'kernel'), ('decoder', 'up_blocks_3', 'resnets_2', 'norm1', 'bias'), ('decoder', 'up_blocks_3', 'resnets_2', 'norm1', 'scale'), ('decoder', 'up_blocks_3', 'resnets_2', 'norm2', 'bias'), ('decoder', 'up_blocks_3', 'resnets_2', 'norm2', 'scale'), ('encoder', 'conv_in', 'bias'), ('encoder', 'conv_in', 'kernel'), ('encoder', 'conv_norm_out', 'bias'), ('encoder', 'conv_norm_out', 'scale'), ('encoder', 'conv_out', 'bias'), ('encoder', 'conv_out', 'kernel'), ('encoder', 'down_blocks_0', 'downsamplers_0', 'conv', 'bias'), ('encoder', 'down_blocks_0', 'downsamplers_0', 'conv', 'kernel'), ('encoder', 'down_blocks_0', 'resnets_0', 'conv1', 'bias'), ('encoder', 'down_blocks_0', 'resnets_0', 'conv1', 'kernel'), ('encoder', 'down_blocks_0', 'resnets_0', 'conv2', 'bias'), ('encoder', 'down_blocks_0', 'resnets_0', 'conv2', 'kernel'), ('encoder', 'down_blocks_0', 'resnets_0', 'norm1', 'bias'), ('encoder', 'down_blocks_0', 'resnets_0', 'norm1', 'scale'), ('encoder', 'down_blocks_0', 'resnets_0', 'norm2', 'bias'), ('encoder', 'down_blocks_0', 'resnets_0', 'norm2', 'scale'), ('encoder', 'down_blocks_0', 'resnets_1', 'conv1', 'bias'), ('encoder', 'down_blocks_0', 'resnets_1', 'conv1', 'kernel'), ('encoder', 'down_blocks_0', 'resnets_1', 'conv2', 'bias'), ('encoder', 'down_blocks_0', 'resnets_1', 'conv2', 'kernel'), ('encoder', 'down_blocks_0', 'resnets_1', 'norm1', 'bias'), ('encoder', 'down_blocks_0', 'resnets_1', 'norm1', 'scale'), ('encoder', 'down_blocks_0', 'resnets_1', 'norm2', 'bias'), ('encoder', 'down_blocks_0', 'resnets_1', 'norm2', 'scale'), ('encoder', 'down_blocks_1', 'downsamplers_0', 'conv', 'bias'), ('encoder', 'down_blocks_1', 'downsamplers_0', 'conv', 'kernel'), ('encoder', 'down_blocks_1', 'resnets_0', 'conv1', 'bias'), ('encoder', 'down_blocks_1', 'resnets_0', 'conv1', 'kernel'), ('encoder', 'down_blocks_1', 'resnets_0', 'conv2', 'bias'), ('encoder', 'down_blocks_1', 'resnets_0', 'conv2', 'kernel'), ('encoder', 'down_blocks_1', 'resnets_0', 'conv_shortcut', 'bias'), ('encoder', 'down_blocks_1', 'resnets_0', 'conv_shortcut', 'kernel'), ('encoder', 'down_blocks_1', 'resnets_0', 'norm1', 'bias'), ('encoder', 'down_blocks_1', 'resnets_0', 'norm1', 'scale'), ('encoder', 'down_blocks_1', 'resnets_0', 'norm2', 'bias'), ('encoder', 'down_blocks_1', 'resnets_0', 'norm2', 'scale'), ('encoder', 'down_blocks_1', 'resnets_1', 'conv1', 'bias'), ('encoder', 'down_blocks_1', 'resnets_1', 'conv1', 'kernel'), ('encoder', 'down_blocks_1', 'resnets_1', 'conv2', 'bias'), ('encoder', 'down_blocks_1', 'resnets_1', 'conv2', 'kernel'), ('encoder', 'down_blocks_1', 'resnets_1', 'norm1', 'bias'), ('encoder', 'down_blocks_1', 'resnets_1', 'norm1', 'scale'), ('encoder', 'down_blocks_1', 'resnets_1', 'norm2', 'bias'), ('encoder', 'down_blocks_1', 'resnets_1', 'norm2', 'scale'), ('encoder', 'down_blocks_2', 'downsamplers_0', 'conv', 'bias'), ('encoder', 'down_blocks_2', 'downsamplers_0', 'conv', 'kernel'), ('encoder', 'down_blocks_2', 'resnets_0', 'conv1', 'bias'), ('encoder', 'down_blocks_2', 'resnets_0', 'conv1', 'kernel'), ('encoder', 'down_blocks_2', 'resnets_0', 'conv2', 'bias'), ('encoder', 'down_blocks_2', 'resnets_0', 'conv2', 'kernel'), ('encoder', 'down_blocks_2', 'resnets_0', 'conv_shortcut', 'bias'), ('encoder', 'down_blocks_2', 'resnets_0', 'conv_shortcut', 'kernel'), ('encoder', 'down_blocks_2', 'resnets_0', 'norm1', 'bias'), ('encoder', 'down_blocks_2', 'resnets_0', 'norm1', 'scale'), ('encoder', 'down_blocks_2', 'resnets_0', 'norm2', 'bias'), ('encoder', 'down_blocks_2', 'resnets_0', 'norm2', 'scale'), ('encoder', 'down_blocks_2', 'resnets_1', 'conv1', 'bias'), ('encoder', 'down_blocks_2', 'resnets_1', 'conv1', 'kernel'), ('encoder', 'down_blocks_2', 'resnets_1', 'conv2', 'bias'), ('encoder', 'down_blocks_2', 'resnets_1', 'conv2', 'kernel'), ('encoder', 'down_blocks_2', 'resnets_1', 'norm1', 'bias'), ('encoder', 'down_blocks_2', 'resnets_1', 'norm1', 'scale'), ('encoder', 'down_blocks_2', 'resnets_1', 'norm2', 'bias'), ('encoder', 'down_blocks_2', 'resnets_1', 'norm2', 'scale'), ('encoder', 'down_blocks_3', 'resnets_0', 'conv1', 'bias'), ('encoder', 'down_blocks_3', 'resnets_0', 'conv1', 'kernel'), ('encoder', 'down_blocks_3', 'resnets_0', 'conv2', 'bias'), ('encoder', 'down_blocks_3', 'resnets_0', 'conv2', 'kernel'), ('encoder', 'down_blocks_3', 'resnets_0', 'norm1', 'bias'), ('encoder', 'down_blocks_3', 'resnets_0', 'norm1', 'scale'), ('encoder', 'down_blocks_3', 'resnets_0', 'norm2', 'bias'), ('encoder', 'down_blocks_3', 'resnets_0', 'norm2', 'scale'), ('encoder', 'down_blocks_3', 'resnets_1', 'conv1', 'bias'), ('encoder', 'down_blocks_3', 'resnets_1', 'conv1', 'kernel'), ('encoder', 'down_blocks_3', 'resnets_1', 'conv2', 'bias'), ('encoder', 'down_blocks_3', 'resnets_1', 'conv2', 'kernel'), ('encoder', 'down_blocks_3', 'resnets_1', 'norm1', 'bias'), ('encoder', 'down_blocks_3', 'resnets_1', 'norm1', 'scale'), ('encoder', 'down_blocks_3', 'resnets_1', 'norm2', 'bias'), ('encoder', 'down_blocks_3', 'resnets_1', 'norm2', 'scale'), ('encoder', 'mid_block', 'attentions_0', 'group_norm', 'bias'), ('encoder', 'mid_block', 'attentions_0', 'group_norm', 'scale'), ('encoder', 'mid_block', 'attentions_0', 'key', 'bias'), ('encoder', 'mid_block', 'attentions_0', 'key', 'kernel'), ('encoder', 'mid_block', 'attentions_0', 'proj_attn', 'bias'), ('encoder', 'mid_block', 'attentions_0', 'proj_attn', 'kernel'), ('encoder', 'mid_block', 'attentions_0', 'query', 'bias'), ('encoder', 'mid_block', 'attentions_0', 'query', 'kernel'), ('encoder', 'mid_block', 'attentions_0', 'value', 'bias'), ('encoder', 'mid_block', 'attentions_0', 'value', 'kernel'), ('encoder', 'mid_block', 'resnets_0', 'conv1', 'bias'), ('encoder', 'mid_block', 'resnets_0', 'conv1', 'kernel'), ('encoder', 'mid_block', 'resnets_0', 'conv2', 'bias'), ('encoder', 'mid_block', 'resnets_0', 'conv2', 'kernel'), ('encoder', 'mid_block', 'resnets_0', 'norm1', 'bias'), ('encoder', 'mid_block', 'resnets_0', 'norm1', 'scale'), ('encoder', 'mid_block', 'resnets_0', 'norm2', 'bias'), ('encoder', 'mid_block', 'resnets_0', 'norm2', 'scale'), ('encoder', 'mid_block', 'resnets_1', 'conv1', 'bias'), ('encoder', 'mid_block', 'resnets_1', 'conv1', 'kernel'), ('encoder', 'mid_block', 'resnets_1', 'conv2', 'bias'), ('encoder', 'mid_block', 'resnets_1', 'conv2', 'kernel'), ('encoder', 'mid_block', 'resnets_1', 'norm1', 'bias'), ('encoder', 'mid_block', 'resnets_1', 'norm1', 'scale'), ('encoder', 'mid_block', 'resnets_1', 'norm2', 'bias'), ('encoder', 'mid_block', 'resnets_1', 'norm2', 'scale'), ('post_quant_conv', 'bias'), ('post_quant_conv', 'kernel'), ('quant_conv', 'bias'), ('quant_conv', 'kernel')]\nYou should probably UPCAST the model weights to float32 if this was not intended. See [`~ModelMixin.to_fp32`] for further information on how to do this.\nSome of the weights of FlaxUNet2DConditionModel were initialized in bfloat16 precision from the model checkpoint at /root/.cache/huggingface/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/295cccdedbd5f87458186972858dc85c7e70c10a/unet:\n[('conv_in', 'bias'), ('conv_in', 'kernel'), ('conv_norm_out', 'bias'), ('conv_norm_out', 'scale'), ('conv_out', 'bias'), ('conv_out', 'kernel'), ('down_blocks_0', 'attentions_0', 'norm', 'bias'), ('down_blocks_0', 'attentions_0', 'norm', 'scale'), ('down_blocks_0', 'attentions_0', 'proj_in', 'bias'), ('down_blocks_0', 'attentions_0', 'proj_in', 'kernel'), ('down_blocks_0', 'attentions_0', 'proj_out', 'bias'), ('down_blocks_0', 'attentions_0', 'proj_out', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'norm1', 'bias'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'norm1', 'scale'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'norm2', 'bias'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'norm2', 'scale'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'norm3', 'bias'), ('down_blocks_0', 'attentions_0', 'transformer_blocks_0', 'norm3', 'scale'), ('down_blocks_0', 'attentions_1', 'norm', 'bias'), ('down_blocks_0', 'attentions_1', 'norm', 'scale'), ('down_blocks_0', 'attentions_1', 'proj_in', 'bias'), ('down_blocks_0', 'attentions_1', 'proj_in', 'kernel'), ('down_blocks_0', 'attentions_1', 'proj_out', 'bias'), ('down_blocks_0', 'attentions_1', 'proj_out', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'norm1', 'bias'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'norm1', 'scale'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'norm2', 'bias'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'norm2', 'scale'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'norm3', 'bias'), ('down_blocks_0', 'attentions_1', 'transformer_blocks_0', 'norm3', 'scale'), ('down_blocks_0', 'downsamplers_0', 'conv', 'bias'), ('down_blocks_0', 'downsamplers_0', 'conv', 'kernel'), ('down_blocks_0', 'resnets_0', 'conv1', 'bias'), ('down_blocks_0', 'resnets_0', 'conv1', 'kernel'), ('down_blocks_0', 'resnets_0', 'conv2', 'bias'), ('down_blocks_0', 'resnets_0', 'conv2', 'kernel'), ('down_blocks_0', 'resnets_0', 'norm1', 'bias'), ('down_blocks_0', 'resnets_0', 'norm1', 'scale'), ('down_blocks_0', 'resnets_0', 'norm2', 'bias'), ('down_blocks_0', 'resnets_0', 'norm2', 'scale'), ('down_blocks_0', 'resnets_0', 'time_emb_proj', 'bias'), ('down_blocks_0', 'resnets_0', 'time_emb_proj', 'kernel'), ('down_blocks_0', 'resnets_1', 'conv1', 'bias'), ('down_blocks_0', 'resnets_1', 'conv1', 'kernel'), ('down_blocks_0', 'resnets_1', 'conv2', 'bias'), ('down_blocks_0', 'resnets_1', 'conv2', 'kernel'), ('down_blocks_0', 'resnets_1', 'norm1', 'bias'), ('down_blocks_0', 'resnets_1', 'norm1', 'scale'), ('down_blocks_0', 'resnets_1', 'norm2', 'bias'), ('down_blocks_0', 'resnets_1', 'norm2', 'scale'), ('down_blocks_0', 'resnets_1', 'time_emb_proj', 'bias'), ('down_blocks_0', 'resnets_1', 'time_emb_proj', 'kernel'), ('down_blocks_1', 'attentions_0', 'norm', 'bias'), ('down_blocks_1', 'attentions_0', 'norm', 'scale'), ('down_blocks_1', 'attentions_0', 'proj_in', 'bias'), ('down_blocks_1', 'attentions_0', 'proj_in', 'kernel'), ('down_blocks_1', 'attentions_0', 'proj_out', 'bias'), ('down_blocks_1', 'attentions_0', 'proj_out', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm1', 'bias'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm1', 'scale'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm2', 'bias'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm2', 'scale'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm3', 'bias'), ('down_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm3', 'scale'), ('down_blocks_1', 'attentions_1', 'norm', 'bias'), ('down_blocks_1', 'attentions_1', 'norm', 'scale'), ('down_blocks_1', 'attentions_1', 'proj_in', 'bias'), ('down_blocks_1', 'attentions_1', 'proj_in', 'kernel'), ('down_blocks_1', 'attentions_1', 'proj_out', 'bias'), ('down_blocks_1', 'attentions_1', 'proj_out', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm1', 'bias'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm1', 'scale'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm2', 'bias'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm2', 'scale'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm3', 'bias'), ('down_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm3', 'scale'), ('down_blocks_1', 'downsamplers_0', 'conv', 'bias'), ('down_blocks_1', 'downsamplers_0', 'conv', 'kernel'), ('down_blocks_1', 'resnets_0', 'conv1', 'bias'), ('down_blocks_1', 'resnets_0', 'conv1', 'kernel'), ('down_blocks_1', 'resnets_0', 'conv2', 'bias'), ('down_blocks_1', 'resnets_0', 'conv2', 'kernel'), ('down_blocks_1', 'resnets_0', 'conv_shortcut', 'bias'), ('down_blocks_1', 'resnets_0', 'conv_shortcut', 'kernel'), ('down_blocks_1', 'resnets_0', 'norm1', 'bias'), ('down_blocks_1', 'resnets_0', 'norm1', 'scale'), ('down_blocks_1', 'resnets_0', 'norm2', 'bias'), ('down_blocks_1', 'resnets_0', 'norm2', 'scale'), ('down_blocks_1', 'resnets_0', 'time_emb_proj', 'bias'), ('down_blocks_1', 'resnets_0', 'time_emb_proj', 'kernel'), ('down_blocks_1', 'resnets_1', 'conv1', 'bias'), ('down_blocks_1', 'resnets_1', 'conv1', 'kernel'), ('down_blocks_1', 'resnets_1', 'conv2', 'bias'), ('down_blocks_1', 'resnets_1', 'conv2', 'kernel'), ('down_blocks_1', 'resnets_1', 'norm1', 'bias'), ('down_blocks_1', 'resnets_1', 'norm1', 'scale'), ('down_blocks_1', 'resnets_1', 'norm2', 'bias'), ('down_blocks_1', 'resnets_1', 'norm2', 'scale'), ('down_blocks_1', 'resnets_1', 'time_emb_proj', 'bias'), ('down_blocks_1', 'resnets_1', 'time_emb_proj', 'kernel'), ('down_blocks_2', 'attentions_0', 'norm', 'bias'), ('down_blocks_2', 'attentions_0', 'norm', 'scale'), ('down_blocks_2', 'attentions_0', 'proj_in', 'bias'), ('down_blocks_2', 'attentions_0', 'proj_in', 'kernel'), ('down_blocks_2', 'attentions_0', 'proj_out', 'bias'), ('down_blocks_2', 'attentions_0', 'proj_out', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm1', 'bias'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm1', 'scale'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm2', 'bias'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm2', 'scale'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm3', 'bias'), ('down_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm3', 'scale'), ('down_blocks_2', 'attentions_1', 'norm', 'bias'), ('down_blocks_2', 'attentions_1', 'norm', 'scale'), ('down_blocks_2', 'attentions_1', 'proj_in', 'bias'), ('down_blocks_2', 'attentions_1', 'proj_in', 'kernel'), ('down_blocks_2', 'attentions_1', 'proj_out', 'bias'), ('down_blocks_2', 'attentions_1', 'proj_out', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm1', 'bias'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm1', 'scale'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm2', 'bias'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm2', 'scale'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm3', 'bias'), ('down_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm3', 'scale'), ('down_blocks_2', 'downsamplers_0', 'conv', 'bias'), ('down_blocks_2', 'downsamplers_0', 'conv', 'kernel'), ('down_blocks_2', 'resnets_0', 'conv1', 'bias'), ('down_blocks_2', 'resnets_0', 'conv1', 'kernel'), ('down_blocks_2', 'resnets_0', 'conv2', 'bias'), ('down_blocks_2', 'resnets_0', 'conv2', 'kernel'), ('down_blocks_2', 'resnets_0', 'conv_shortcut', 'bias'), ('down_blocks_2', 'resnets_0', 'conv_shortcut', 'kernel'), ('down_blocks_2', 'resnets_0', 'norm1', 'bias'), ('down_blocks_2', 'resnets_0', 'norm1', 'scale'), ('down_blocks_2', 'resnets_0', 'norm2', 'bias'), ('down_blocks_2', 'resnets_0', 'norm2', 'scale'), ('down_blocks_2', 'resnets_0', 'time_emb_proj', 'bias'), ('down_blocks_2', 'resnets_0', 'time_emb_proj', 'kernel'), ('down_blocks_2', 'resnets_1', 'conv1', 'bias'), ('down_blocks_2', 'resnets_1', 'conv1', 'kernel'), ('down_blocks_2', 'resnets_1', 'conv2', 'bias'), ('down_blocks_2', 'resnets_1', 'conv2', 'kernel'), ('down_blocks_2', 'resnets_1', 'norm1', 'bias'), ('down_blocks_2', 'resnets_1', 'norm1', 'scale'), ('down_blocks_2', 'resnets_1', 'norm2', 'bias'), ('down_blocks_2', 'resnets_1', 'norm2', 'scale'), ('down_blocks_2', 'resnets_1', 'time_emb_proj', 'bias'), ('down_blocks_2', 'resnets_1', 'time_emb_proj', 'kernel'), ('down_blocks_3', 'resnets_0', 'conv1', 'bias'), ('down_blocks_3', 'resnets_0', 'conv1', 'kernel'), ('down_blocks_3', 'resnets_0', 'conv2', 'bias'), ('down_blocks_3', 'resnets_0', 'conv2', 'kernel'), ('down_blocks_3', 'resnets_0', 'norm1', 'bias'), ('down_blocks_3', 'resnets_0', 'norm1', 'scale'), ('down_blocks_3', 'resnets_0', 'norm2', 'bias'), ('down_blocks_3', 'resnets_0', 'norm2', 'scale'), ('down_blocks_3', 'resnets_0', 'time_emb_proj', 'bias'), ('down_blocks_3', 'resnets_0', 'time_emb_proj', 'kernel'), ('down_blocks_3', 'resnets_1', 'conv1', 'bias'), ('down_blocks_3', 'resnets_1', 'conv1', 'kernel'), ('down_blocks_3', 'resnets_1', 'conv2', 'bias'), ('down_blocks_3', 'resnets_1', 'conv2', 'kernel'), ('down_blocks_3', 'resnets_1', 'norm1', 'bias'), ('down_blocks_3', 'resnets_1', 'norm1', 'scale'), ('down_blocks_3', 'resnets_1', 'norm2', 'bias'), ('down_blocks_3', 'resnets_1', 'norm2', 'scale'), ('down_blocks_3', 'resnets_1', 'time_emb_proj', 'bias'), ('down_blocks_3', 'resnets_1', 'time_emb_proj', 'kernel'), ('mid_block', 'attentions_0', 'norm', 'bias'), ('mid_block', 'attentions_0', 'norm', 'scale'), ('mid_block', 'attentions_0', 'proj_in', 'bias'), ('mid_block', 'attentions_0', 'proj_in', 'kernel'), ('mid_block', 'attentions_0', 'proj_out', 'bias'), ('mid_block', 'attentions_0', 'proj_out', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'norm1', 'bias'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'norm1', 'scale'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'norm2', 'bias'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'norm2', 'scale'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'norm3', 'bias'), ('mid_block', 'attentions_0', 'transformer_blocks_0', 'norm3', 'scale'), ('mid_block', 'resnets_0', 'conv1', 'bias'), ('mid_block', 'resnets_0', 'conv1', 'kernel'), ('mid_block', 'resnets_0', 'conv2', 'bias'), ('mid_block', 'resnets_0', 'conv2', 'kernel'), ('mid_block', 'resnets_0', 'norm1', 'bias'), ('mid_block', 'resnets_0', 'norm1', 'scale'), ('mid_block', 'resnets_0', 'norm2', 'bias'), ('mid_block', 'resnets_0', 'norm2', 'scale'), ('mid_block', 'resnets_0', 'time_emb_proj', 'bias'), ('mid_block', 'resnets_0', 'time_emb_proj', 'kernel'), ('mid_block', 'resnets_1', 'conv1', 'bias'), ('mid_block', 'resnets_1', 'conv1', 'kernel'), ('mid_block', 'resnets_1', 'conv2', 'bias'), ('mid_block', 'resnets_1', 'conv2', 'kernel'), ('mid_block', 'resnets_1', 'norm1', 'bias'), ('mid_block', 'resnets_1', 'norm1', 'scale'), ('mid_block', 'resnets_1', 'norm2', 'bias'), ('mid_block', 'resnets_1', 'norm2', 'scale'), ('mid_block', 'resnets_1', 'time_emb_proj', 'bias'), ('mid_block', 'resnets_1', 'time_emb_proj', 'kernel'), ('time_embedding', 'linear_1', 'bias'), ('time_embedding', 'linear_1', 'kernel'), ('time_embedding', 'linear_2', 'bias'), ('time_embedding', 'linear_2', 'kernel'), ('up_blocks_0', 'resnets_0', 'conv1', 'bias'), ('up_blocks_0', 'resnets_0', 'conv1', 'kernel'), ('up_blocks_0', 'resnets_0', 'conv2', 'bias'), ('up_blocks_0', 'resnets_0', 'conv2', 'kernel'), ('up_blocks_0', 'resnets_0', 'conv_shortcut', 'bias'), ('up_blocks_0', 'resnets_0', 'conv_shortcut', 'kernel'), ('up_blocks_0', 'resnets_0', 'norm1', 'bias'), ('up_blocks_0', 'resnets_0', 'norm1', 'scale'), ('up_blocks_0', 'resnets_0', 'norm2', 'bias'), ('up_blocks_0', 'resnets_0', 'norm2', 'scale'), ('up_blocks_0', 'resnets_0', 'time_emb_proj', 'bias'), ('up_blocks_0', 'resnets_0', 'time_emb_proj', 'kernel'), ('up_blocks_0', 'resnets_1', 'conv1', 'bias'), ('up_blocks_0', 'resnets_1', 'conv1', 'kernel'), ('up_blocks_0', 'resnets_1', 'conv2', 'bias'), ('up_blocks_0', 'resnets_1', 'conv2', 'kernel'), ('up_blocks_0', 'resnets_1', 'conv_shortcut', 'bias'), ('up_blocks_0', 'resnets_1', 'conv_shortcut', 'kernel'), ('up_blocks_0', 'resnets_1', 'norm1', 'bias'), ('up_blocks_0', 'resnets_1', 'norm1', 'scale'), ('up_blocks_0', 'resnets_1', 'norm2', 'bias'), ('up_blocks_0', 'resnets_1', 'norm2', 'scale'), ('up_blocks_0', 'resnets_1', 'time_emb_proj', 'bias'), ('up_blocks_0', 'resnets_1', 'time_emb_proj', 'kernel'), ('up_blocks_0', 'resnets_2', 'conv1', 'bias'), ('up_blocks_0', 'resnets_2', 'conv1', 'kernel'), ('up_blocks_0', 'resnets_2', 'conv2', 'bias'), ('up_blocks_0', 'resnets_2', 'conv2', 'kernel'), ('up_blocks_0', 'resnets_2', 'conv_shortcut', 'bias'), ('up_blocks_0', 'resnets_2', 'conv_shortcut', 'kernel'), ('up_blocks_0', 'resnets_2', 'norm1', 'bias'), ('up_blocks_0', 'resnets_2', 'norm1', 'scale'), ('up_blocks_0', 'resnets_2', 'norm2', 'bias'), ('up_blocks_0', 'resnets_2', 'norm2', 'scale'), ('up_blocks_0', 'resnets_2', 'time_emb_proj', 'bias'), ('up_blocks_0', 'resnets_2', 'time_emb_proj', 'kernel'), ('up_blocks_0', 'upsamplers_0', 'conv', 'bias'), ('up_blocks_0', 'upsamplers_0', 'conv', 'kernel'), ('up_blocks_1', 'attentions_0', 'norm', 'bias'), ('up_blocks_1', 'attentions_0', 'norm', 'scale'), ('up_blocks_1', 'attentions_0', 'proj_in', 'bias'), ('up_blocks_1', 'attentions_0', 'proj_in', 'kernel'), ('up_blocks_1', 'attentions_0', 'proj_out', 'bias'), ('up_blocks_1', 'attentions_0', 'proj_out', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_1', 'attentions_0', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_1', 'attentions_1', 'norm', 'bias'), ('up_blocks_1', 'attentions_1', 'norm', 'scale'), ('up_blocks_1', 'attentions_1', 'proj_in', 'bias'), ('up_blocks_1', 'attentions_1', 'proj_in', 'kernel'), ('up_blocks_1', 'attentions_1', 'proj_out', 'bias'), ('up_blocks_1', 'attentions_1', 'proj_out', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_1', 'attentions_1', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_1', 'attentions_2', 'norm', 'bias'), ('up_blocks_1', 'attentions_2', 'norm', 'scale'), ('up_blocks_1', 'attentions_2', 'proj_in', 'bias'), ('up_blocks_1', 'attentions_2', 'proj_in', 'kernel'), ('up_blocks_1', 'attentions_2', 'proj_out', 'bias'), ('up_blocks_1', 'attentions_2', 'proj_out', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_1', 'attentions_2', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_1', 'resnets_0', 'conv1', 'bias'), ('up_blocks_1', 'resnets_0', 'conv1', 'kernel'), ('up_blocks_1', 'resnets_0', 'conv2', 'bias'), ('up_blocks_1', 'resnets_0', 'conv2', 'kernel'), ('up_blocks_1', 'resnets_0', 'conv_shortcut', 'bias'), ('up_blocks_1', 'resnets_0', 'conv_shortcut', 'kernel'), ('up_blocks_1', 'resnets_0', 'norm1', 'bias'), ('up_blocks_1', 'resnets_0', 'norm1', 'scale'), ('up_blocks_1', 'resnets_0', 'norm2', 'bias'), ('up_blocks_1', 'resnets_0', 'norm2', 'scale'), ('up_blocks_1', 'resnets_0', 'time_emb_proj', 'bias'), ('up_blocks_1', 'resnets_0', 'time_emb_proj', 'kernel'), ('up_blocks_1', 'resnets_1', 'conv1', 'bias'), ('up_blocks_1', 'resnets_1', 'conv1', 'kernel'), ('up_blocks_1', 'resnets_1', 'conv2', 'bias'), ('up_blocks_1', 'resnets_1', 'conv2', 'kernel'), ('up_blocks_1', 'resnets_1', 'conv_shortcut', 'bias'), ('up_blocks_1', 'resnets_1', 'conv_shortcut', 'kernel'), ('up_blocks_1', 'resnets_1', 'norm1', 'bias'), ('up_blocks_1', 'resnets_1', 'norm1', 'scale'), ('up_blocks_1', 'resnets_1', 'norm2', 'bias'), ('up_blocks_1', 'resnets_1', 'norm2', 'scale'), ('up_blocks_1', 'resnets_1', 'time_emb_proj', 'bias'), ('up_blocks_1', 'resnets_1', 'time_emb_proj', 'kernel'), ('up_blocks_1', 'resnets_2', 'conv1', 'bias'), ('up_blocks_1', 'resnets_2', 'conv1', 'kernel'), ('up_blocks_1', 'resnets_2', 'conv2', 'bias'), ('up_blocks_1', 'resnets_2', 'conv2', 'kernel'), ('up_blocks_1', 'resnets_2', 'conv_shortcut', 'bias'), ('up_blocks_1', 'resnets_2', 'conv_shortcut', 'kernel'), ('up_blocks_1', 'resnets_2', 'norm1', 'bias'), ('up_blocks_1', 'resnets_2', 'norm1', 'scale'), ('up_blocks_1', 'resnets_2', 'norm2', 'bias'), ('up_blocks_1', 'resnets_2', 'norm2', 'scale'), ('up_blocks_1', 'resnets_2', 'time_emb_proj', 'bias'), ('up_blocks_1', 'resnets_2', 'time_emb_proj', 'kernel'), ('up_blocks_1', 'upsamplers_0', 'conv', 'bias'), ('up_blocks_1', 'upsamplers_0', 'conv', 'kernel'), ('up_blocks_2', 'attentions_0', 'norm', 'bias'), ('up_blocks_2', 'attentions_0', 'norm', 'scale'), ('up_blocks_2', 'attentions_0', 'proj_in', 'bias'), ('up_blocks_2', 'attentions_0', 'proj_in', 'kernel'), ('up_blocks_2', 'attentions_0', 'proj_out', 'bias'), ('up_blocks_2', 'attentions_0', 'proj_out', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_2', 'attentions_0', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_2', 'attentions_1', 'norm', 'bias'), ('up_blocks_2', 'attentions_1', 'norm', 'scale'), ('up_blocks_2', 'attentions_1', 'proj_in', 'bias'), ('up_blocks_2', 'attentions_1', 'proj_in', 'kernel'), ('up_blocks_2', 'attentions_1', 'proj_out', 'bias'), ('up_blocks_2', 'attentions_1', 'proj_out', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_2', 'attentions_1', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_2', 'attentions_2', 'norm', 'bias'), ('up_blocks_2', 'attentions_2', 'norm', 'scale'), ('up_blocks_2', 'attentions_2', 'proj_in', 'bias'), ('up_blocks_2', 'attentions_2', 'proj_in', 'kernel'), ('up_blocks_2', 'attentions_2', 'proj_out', 'bias'), ('up_blocks_2', 'attentions_2', 'proj_out', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_2', 'attentions_2', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_2', 'resnets_0', 'conv1', 'bias'), ('up_blocks_2', 'resnets_0', 'conv1', 'kernel'), ('up_blocks_2', 'resnets_0', 'conv2', 'bias'), ('up_blocks_2', 'resnets_0', 'conv2', 'kernel'), ('up_blocks_2', 'resnets_0', 'conv_shortcut', 'bias'), ('up_blocks_2', 'resnets_0', 'conv_shortcut', 'kernel'), ('up_blocks_2', 'resnets_0', 'norm1', 'bias'), ('up_blocks_2', 'resnets_0', 'norm1', 'scale'), ('up_blocks_2', 'resnets_0', 'norm2', 'bias'), ('up_blocks_2', 'resnets_0', 'norm2', 'scale'), ('up_blocks_2', 'resnets_0', 'time_emb_proj', 'bias'), ('up_blocks_2', 'resnets_0', 'time_emb_proj', 'kernel'), ('up_blocks_2', 'resnets_1', 'conv1', 'bias'), ('up_blocks_2', 'resnets_1', 'conv1', 'kernel'), ('up_blocks_2', 'resnets_1', 'conv2', 'bias'), ('up_blocks_2', 'resnets_1', 'conv2', 'kernel'), ('up_blocks_2', 'resnets_1', 'conv_shortcut', 'bias'), ('up_blocks_2', 'resnets_1', 'conv_shortcut', 'kernel'), ('up_blocks_2', 'resnets_1', 'norm1', 'bias'), ('up_blocks_2', 'resnets_1', 'norm1', 'scale'), ('up_blocks_2', 'resnets_1', 'norm2', 'bias'), ('up_blocks_2', 'resnets_1', 'norm2', 'scale'), ('up_blocks_2', 'resnets_1', 'time_emb_proj', 'bias'), ('up_blocks_2', 'resnets_1', 'time_emb_proj', 'kernel'), ('up_blocks_2', 'resnets_2', 'conv1', 'bias'), ('up_blocks_2', 'resnets_2', 'conv1', 'kernel'), ('up_blocks_2', 'resnets_2', 'conv2', 'bias'), ('up_blocks_2', 'resnets_2', 'conv2', 'kernel'), ('up_blocks_2', 'resnets_2', 'conv_shortcut', 'bias'), ('up_blocks_2', 'resnets_2', 'conv_shortcut', 'kernel'), ('up_blocks_2', 'resnets_2', 'norm1', 'bias'), ('up_blocks_2', 'resnets_2', 'norm1', 'scale'), ('up_blocks_2', 'resnets_2', 'norm2', 'bias'), ('up_blocks_2', 'resnets_2', 'norm2', 'scale'), ('up_blocks_2', 'resnets_2', 'time_emb_proj', 'bias'), ('up_blocks_2', 'resnets_2', 'time_emb_proj', 'kernel'), ('up_blocks_2', 'upsamplers_0', 'conv', 'bias'), ('up_blocks_2', 'upsamplers_0', 'conv', 'kernel'), ('up_blocks_3', 'attentions_0', 'norm', 'bias'), ('up_blocks_3', 'attentions_0', 'norm', 'scale'), ('up_blocks_3', 'attentions_0', 'proj_in', 'bias'), ('up_blocks_3', 'attentions_0', 'proj_in', 'kernel'), ('up_blocks_3', 'attentions_0', 'proj_out', 'bias'), ('up_blocks_3', 'attentions_0', 'proj_out', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_3', 'attentions_0', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_3', 'attentions_1', 'norm', 'bias'), ('up_blocks_3', 'attentions_1', 'norm', 'scale'), ('up_blocks_3', 'attentions_1', 'proj_in', 'bias'), ('up_blocks_3', 'attentions_1', 'proj_in', 'kernel'), ('up_blocks_3', 'attentions_1', 'proj_out', 'bias'), ('up_blocks_3', 'attentions_1', 'proj_out', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_3', 'attentions_1', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_3', 'attentions_2', 'norm', 'bias'), ('up_blocks_3', 'attentions_2', 'norm', 'scale'), ('up_blocks_3', 'attentions_2', 'proj_in', 'bias'), ('up_blocks_3', 'attentions_2', 'proj_in', 'kernel'), ('up_blocks_3', 'attentions_2', 'proj_out', 'bias'), ('up_blocks_3', 'attentions_2', 'proj_out', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_k', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_out_0', 'bias'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_out_0', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_q', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn1', 'to_v', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_k', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_out_0', 'bias'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_out_0', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_q', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'attn2', 'to_v', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'bias'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_0', 'proj', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_2', 'bias'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'ff', 'net_2', 'kernel'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'norm1', 'bias'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'norm1', 'scale'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'norm2', 'bias'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'norm2', 'scale'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'norm3', 'bias'), ('up_blocks_3', 'attentions_2', 'transformer_blocks_0', 'norm3', 'scale'), ('up_blocks_3', 'resnets_0', 'conv1', 'bias'), ('up_blocks_3', 'resnets_0', 'conv1', 'kernel'), ('up_blocks_3', 'resnets_0', 'conv2', 'bias'), ('up_blocks_3', 'resnets_0', 'conv2', 'kernel'), ('up_blocks_3', 'resnets_0', 'conv_shortcut', 'bias'), ('up_blocks_3', 'resnets_0', 'conv_shortcut', 'kernel'), ('up_blocks_3', 'resnets_0', 'norm1', 'bias'), ('up_blocks_3', 'resnets_0', 'norm1', 'scale'), ('up_blocks_3', 'resnets_0', 'norm2', 'bias'), ('up_blocks_3', 'resnets_0', 'norm2', 'scale'), ('up_blocks_3', 'resnets_0', 'time_emb_proj', 'bias'), ('up_blocks_3', 'resnets_0', 'time_emb_proj', 'kernel'), ('up_blocks_3', 'resnets_1', 'conv1', 'bias'), ('up_blocks_3', 'resnets_1', 'conv1', 'kernel'), ('up_blocks_3', 'resnets_1', 'conv2', 'bias'), ('up_blocks_3', 'resnets_1', 'conv2', 'kernel'), ('up_blocks_3', 'resnets_1', 'conv_shortcut', 'bias'), ('up_blocks_3', 'resnets_1', 'conv_shortcut', 'kernel'), ('up_blocks_3', 'resnets_1', 'norm1', 'bias'), ('up_blocks_3', 'resnets_1', 'norm1', 'scale'), ('up_blocks_3', 'resnets_1', 'norm2', 'bias'), ('up_blocks_3', 'resnets_1', 'norm2', 'scale'), ('up_blocks_3', 'resnets_1', 'time_emb_proj', 'bias'), ('up_blocks_3', 'resnets_1', 'time_emb_proj', 'kernel'), ('up_blocks_3', 'resnets_2', 'conv1', 'bias'), ('up_blocks_3', 'resnets_2', 'conv1', 'kernel'), ('up_blocks_3', 'resnets_2', 'conv2', 'bias'), ('up_blocks_3', 'resnets_2', 'conv2', 'kernel'), ('up_blocks_3', 'resnets_2', 'conv_shortcut', 'bias'), ('up_blocks_3', 'resnets_2', 'conv_shortcut', 'kernel'), ('up_blocks_3', 'resnets_2', 'norm1', 'bias'), ('up_blocks_3', 'resnets_2', 'norm1', 'scale'), ('up_blocks_3', 'resnets_2', 'norm2', 'bias'), ('up_blocks_3', 'resnets_2', 'norm2', 'scale'), ('up_blocks_3', 'resnets_2', 'time_emb_proj', 'bias'), ('up_blocks_3', 'resnets_2', 'time_emb_proj', 'kernel')]\nYou should probably UPCAST the model weights to float32 if this was not intended. See [`~ModelMixin.to_fp32`] for further information on how to do this."
  },
  {
    "objectID": "stable_diffusion.html#inference",
    "href": "stable_diffusion.html#inference",
    "title": "9  Stable Diffusion in JAX / Flax !",
    "section": "9.3 Inference",
    "text": "9.3 Inference\nSince TPUs usually have 8 devices working in parallel, we’ll replicate our prompt as many times as devices we have. Then we’ll perform inference on the 8 devices at once, each responsible for generating one image. Thus, we’ll get 8 images in the same amount of time it takes for one chip to generate a single one.\nAfter replicating the prompt, we obtain the tokenized text ids by invoking the prepare_inputs function of the pipeline. The length of the tokenized text is set to 77 tokens, as required by the configuration of the underlying CLIP Text model.\n\nprompt = \"A cinematic film still of Morgan Freeman starring as Jimi Hendrix, portrait, 40mm lens, shallow depth of field, close up, split lighting, cinematic\"\nprompt = [prompt] * jax.device_count()\nprompt_ids = pipeline.prepare_inputs(prompt)\nprompt_ids.shape\n\n(8, 77)\n\n\n\n9.3.1 Replication and parallelization\nModel parameters and inputs have to be replicated across the 8 parallel devices we have. The parameters dictionary is replicated using flax.jax_utils.replicate, which traverses the dictionary and changes the shape of the weights so they are repeated 8 times. Arrays are replicated using shard.\n\np_params = replicate(params)\n\n\nprompt_ids = shard(prompt_ids)\nprompt_ids.shape\n\n(8, 1, 77)\n\n\nThat shape means that each one of the 8 devices will receive as an input a jnp array with shape (1, 77). 1 is therefore the batch size per device. In TPUs with sufficient memory, it could be larger than 1 if we wanted to generate multiple images (per chip) at once.\nWe are almost ready to generate images! We just need to create a random number generator to pass to the generation function. This is the standard procedure in Flax, which is very serious and opinionated about random numbers – all functions that deal with random numbers are expected to receive a generator. This ensures reproducibility, even when we are training across multiple distributed devices.\nThe helper function below uses a seed to initialize a random number generator. As long as we use the same seed, we’ll get the exact same results. Feel free to use different seeds when exploring results later in the notebook.\n\ndef create_key(seed=0):\n    return jax.random.PRNGKey(seed)\n\nWe obtain a rng and then “split” it 8 times so each device receives a different generator. Therefore, each device will create a different image, and the full process is reproducible.\n\nrng = create_key(0)\nrng = jax.random.split(rng, jax.device_count())\n\nJAX code can be compiled to an efficient representation that runs very fast. However, we need to ensure that all inputs have the same shape in subsequent calls; otherwise, JAX will have to recompile the code, and we wouldn’t be able to take advantage of the optimized speed.\nThe Flax pipeline can compile the code for us if we pass jit = True as an argument. It will also ensure that the model runs in parallel in the 8 available devices.\nThe first time we run the following cell it will take a long time to compile, but subequent calls (even with different inputs) will be much faster. For example, it took more than a minute to compile in a TPU v2-8 when I tested, but then it takes about 7s for future inference runs.\n\n%%time\n\nimages = pipeline(prompt_ids, p_params, rng, jit=True)[0]\n\nCPU times: user 56.2 s, sys: 42.5 s, total: 1min 38s\nWall time: 1min 29s\n\n\nThe returned array has shape (8, 1, 512, 512, 3). We reshape it to get rid of the second dimension and obtain 8 images of 512 × 512 × 3 and then convert them to PIL.\n\nimages = images.reshape((images.shape[0] * images.shape[1], ) + images.shape[-3:])\nimages = pipeline.numpy_to_pil(images)\n\n\n\n9.3.2 Visualization\nLet’s create a helper function to display images in a grid.\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n\nimage_grid(images, 2, 4)"
  },
  {
    "objectID": "stable_diffusion.html#using-different-prompts",
    "href": "stable_diffusion.html#using-different-prompts",
    "title": "9  Stable Diffusion in JAX / Flax !",
    "section": "9.4 Using different prompts",
    "text": "9.4 Using different prompts\nWe don’t have to replicate the same prompt in all the devices. We can do whatever we want: generate 2 prompts 4 times each, or even generate 8 different prompts at once. Let’s do that!\nFirst, we’ll refactor the input preparation code into a handy function:\n\nprompts = [\n    \"Labrador in the style of Hokusai\",\n    \"Painting of a squirrel skating in New York\",\n    \"HAL-9000 in the style of Van Gogh\",\n    \"Times Square under water, with fish and a dolphin swimming around\",\n    \"Ancient Roman fresco showing a man working on his laptop\",\n    \"Close-up photograph of young black woman against urban background, high quality, bokeh\",\n    \"Armchair in the shape of an avocado\",\n    \"Clown astronaut in space, with Earth in the background\"\n]\n\n\nprompt_ids = pipeline.prepare_inputs(prompts)\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, p_params, rng, jit=True).images\nimages = images.reshape((images.shape[0] * images.shape[1], ) + images.shape[-3:])\nimages = pipeline.numpy_to_pil(images)\n\nimage_grid(images, 2, 4)"
  },
  {
    "objectID": "stable_diffusion.html#how-does-parallelization-work",
    "href": "stable_diffusion.html#how-does-parallelization-work",
    "title": "9  Stable Diffusion in JAX / Flax !",
    "section": "9.5 How does parallelization work?",
    "text": "9.5 How does parallelization work?\nWe said before that the diffusers Flax pipeline automatically compiles the model and runs it in parallel on all available devices. We’ll now briefly look inside that process to show how it works.\nJAX parallelization can be done in multiple ways. The easiest one revolves around using the jax.pmap function to achieve single-program, multiple-data (SPMD) parallelization. It means we’ll run several copies of the same code, each on different data inputs. More sophisticated approaches are possible, we invite you to go over the JAX documentation and the pjit pages to explore this topic if you are interested!\njax.pmap does two things for us: - Compiles (or jits) the code, as if we had invoked jax.jit(). This does not happen when we call pmap, but the first time the pmapped function is invoked. - Ensures the compiled code runs in parallel in all the available devices.\nTo show how it works we pmap the _generate method of the pipeline, which is the private method that runs generates images. Please, note that this method may be renamed or removed in future releases of diffusers.\n\np_generate = pmap(pipeline._generate)\n\nAfter we use pmap, the prepared function p_generate will conceptually do the following: * Invoke a copy of the underlying function pipeline._generate in each device. * Send each device a different portion of the input arguments. That’s what sharding is used for. In our case, prompt_ids has shape (8, 1, 77, 768). This array will be split in 8 and each copy of _generate will receive an input with shape (1, 77, 768).\nWe can code _generate completely ignoring the fact that it will be invoked in parallel. We just care about our batch size (1 in this example) and the dimensions that make sense for our code, and don’t have to change anything to make it work in parallel.\nThe same way as when we used the pipeline call, the first time we run the following cell it will take a while, but then it will be much faster.\n\n%%time\n\nimages = p_generate(prompt_ids, p_params, rng)\nimages = images.block_until_ready()\nimages.shape\n\nCPU times: user 1min 15s, sys: 18.2 s, total: 1min 34s\nWall time: 1min 15s\n\n\n(8, 1, 512, 512, 3)\n\n\n\nimages.shape\n\n(8, 1, 512, 512, 3)\n\n\nWe use block_until_ready() to correctly measure inference time, because JAX uses asynchronous dispatch and returns control to the Python loop as soon as it can. You don’t need to use that in your code; blocking will occur automatically when you want to use the result of a computation that has not yet been materialized."
  },
  {
    "objectID": "vit_jax.html",
    "href": "vit_jax.html",
    "title": "10  Vision Transformer",
    "section": "",
    "text": "Colab with minor adjustments from Google Research: https://github.com/google-research/vision_transformer/\nSee papers at\n\nVision Transformer: https://arxiv.org/abs/2010.11929\nMLP-Mixer: https://arxiv.org/abs/2105.01601\nHow to train your ViT: https://arxiv.org/abs/2106.10270\nWhen Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations: https://arxiv.org/abs/2106.01548\n\nThis Colab allows you to run the JAX implementation of the Vision Transformer.\nIf you just want to load a pre-trained checkpoint from a large repository and directly use it for inference, you probably want to go this Colab.\n\n\nImages are split into “patches”\nPatch embeddings: get linear embeddings from the patches. These is also often called “representations”.\nFrom the patch embeddings, add positional embeddings to derive the relevance of the patches to one another and how they combine to form an image. A [cls] token is also added, as with language models. These are included so that information is carried into positional encodings, helping the model understand its relation to other tokens and spatial relationships in the image.\nSend these embeddings through a Transformer encoder, generate output values for the [cls] tokens.\nThese representations / embeddings of [cls] tokens are then sent through the head of a multi-layer perceptron neural net to get class predictions.\n\n\n\n10.0.0.0.1 Copyright 2021 Google LLC.\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n  \n\n\n10.0.1 Setup\nNeeds to be executed once in every VM.\nThe cell below downloads the code from Github and install necessary dependencies.\n\n#@markdown Select whether you would like to store data in your personal drive.\n#@markdown\n#@markdown If you select **yes**, you will need to authorize Colab to access\n#@markdown your personal drive\n#@markdown\n#@markdown If you select **no**, then any changes you make will diappear when\n#@markdown this Colab's VM restarts after some time of inactivity...\nuse_gdrive = 'no'  #@param [\"yes\", \"no\"]\n\nif use_gdrive == 'yes':\n  from google.colab import drive\n  drive.mount('/gdrive')\n  root = '/gdrive/My Drive/vision_transformer_colab'\n  import os\n  if not os.path.isdir(root):\n    os.mkdir(root)\n  os.chdir(root)\n  print(f'\\nChanged CWD to \"{root}\"')\nelse:\n  from IPython import display\n  display.display(display.HTML(\n      '&lt;h1 style=\"color:red\"&gt;CHANGES NOT PERSISTED&lt;/h1&gt;'))\n\nCHANGES NOT PERSISTED\n\n\n\n# Clone repository and pull latest changes.\n![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer\n!cd vision_transformer && git pull\n\nCloning into 'vision_transformer'...\nremote: Enumerating objects: 49, done.\nremote: Counting objects: 100% (49/49), done.\nremote: Compressing objects: 100% (44/44), done.\nremote: Total 49 (delta 6), reused 18 (delta 1), pack-reused 0\nUnpacking objects: 100% (49/49), 1.86 MiB | 6.00 MiB/s, done.\nAlready up to date.\n\n\n\n!pip install -qr vision_transformer/vit_jax/requirements.txt\n\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.2/98.2 kB 7.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 4.8 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 10.1 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 231.8/231.8 MB 2.5 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 106.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 390.5/390.5 kB 36.0 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 83.0 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 63.0 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 62.4 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 MB 6.8 MB/s eta 0:00:00\n  Building wheel for flaxformer (setup.py) ... done\n  Building wheel for ml-collections (setup.py) ... done\n  Building wheel for jax (pyproject.toml) ... done\n\n\n\n\n10.0.2 Imports\n\n# Shows all available pre-trained models.\n!gsutil ls -lh gs://vit_models/imagenet*\n!gsutil ls -lh gs://vit_models/sam\n!gsutil ls -lh gs://mixer_models/*\n\ngs://vit_models/imagenet21k+imagenet2012/:\n377.57 MiB  2020-11-30T16:17:02Z  gs://vit_models/imagenet21k+imagenet2012/R50+ViT-B_16.npz\n330.29 MiB  2020-10-29T17:05:52Z  gs://vit_models/imagenet21k+imagenet2012/ViT-B_16-224.npz\n 331.4 MiB  2020-10-20T11:48:22Z  gs://vit_models/imagenet21k+imagenet2012/ViT-B_16.npz\n336.89 MiB  2020-10-20T11:47:36Z  gs://vit_models/imagenet21k+imagenet2012/ViT-B_32.npz\n334.78 MiB  2021-03-12T09:04:16Z  gs://vit_models/imagenet21k+imagenet2012/ViT-B_8.npz\n  1.13 GiB  2020-10-29T17:08:31Z  gs://vit_models/imagenet21k+imagenet2012/ViT-L_16-224.npz\n  1.14 GiB  2020-10-20T11:53:44Z  gs://vit_models/imagenet21k+imagenet2012/ViT-L_16.npz\n  1.14 GiB  2020-10-20T11:50:56Z  gs://vit_models/imagenet21k+imagenet2012/ViT-L_32.npz\n\ngs://vit_models/imagenet21k/:\n450.23 MiB  2021-01-20T14:12:43Z  gs://vit_models/imagenet21k/R26+ViT-B_32.npz\n439.85 MiB  2020-11-30T10:10:15Z  gs://vit_models/imagenet21k/R50+ViT-B_16.npz\n  1.31 GiB  2021-01-20T14:11:54Z  gs://vit_models/imagenet21k/R50+ViT-L_32.npz\n393.69 MiB  2020-10-22T21:38:39Z  gs://vit_models/imagenet21k/ViT-B_16.npz\n400.01 MiB  2020-11-02T08:30:56Z  gs://vit_models/imagenet21k/ViT-B_32.npz\n393.72 MiB  2021-03-10T13:28:28Z  gs://vit_models/imagenet21k/ViT-B_8.npz\n  2.46 GiB  2020-11-03T10:46:11Z  gs://vit_models/imagenet21k/ViT-H_14.npz\n  1.22 GiB  2020-11-09T14:39:51Z  gs://vit_models/imagenet21k/ViT-L_16.npz\n  1.23 GiB  2020-11-02T08:35:10Z  gs://vit_models/imagenet21k/ViT-L_32.npz\nTOTAL: 17 objects, 14306096550 bytes (13.32 GiB)\n 330.3 MiB  2021-07-13T19:39:09Z  gs://vit_models/sam/ViT-B_16.npz\n336.61 MiB  2021-07-13T19:39:10Z  gs://vit_models/sam/ViT-B_32.npz\n  1.13 GiB  2021-07-13T19:39:38Z  gs://vit_models/sam/ViT-L_16.npz\n  1.14 GiB  2021-07-13T19:39:38Z  gs://vit_models/sam/ViT-L_32.npz\n252.57 MiB  2022-02-18T06:43:52Z  gs://vit_models/sam/ViT-S_16.npz\nTOTAL: 5 objects, 3407859850 bytes (3.17 GiB)\n       6 B  2021-06-28T13:07:12Z  gs://mixer_models/sam_$folder$\n\ngs://mixer_models/gsam/:\n228.47 MiB  2022-07-21T05:47:43Z  gs://mixer_models/gsam/Mixer-B_16.npz\n230.04 MiB  2022-07-21T05:48:18Z  gs://mixer_models/gsam/Mixer-B_32.npz\n 70.71 MiB  2022-07-21T05:48:30Z  gs://mixer_models/gsam/Mixer-S_16.npz\n 72.91 MiB  2022-07-21T05:48:42Z  gs://mixer_models/gsam/Mixer-S_32.npz\n 78.79 MiB  2022-07-21T05:48:56Z  gs://mixer_models/gsam/Mixer-S_8.npz\n\ngs://mixer_models/imagenet1k/:\n228.47 MiB  2021-05-05T14:09:01Z  gs://mixer_models/imagenet1k/Mixer-B_16.npz\n794.29 MiB  2021-05-05T14:09:02Z  gs://mixer_models/imagenet1k/Mixer-L_16.npz\n\ngs://mixer_models/imagenet21k/:\n289.61 MiB  2021-05-05T14:09:11Z  gs://mixer_models/imagenet21k/Mixer-B_16.npz\n875.78 MiB  2021-05-05T14:09:12Z  gs://mixer_models/imagenet21k/Mixer-L_16.npz\n\ngs://mixer_models/sam/:\n228.47 MiB  2021-06-28T13:08:09Z  gs://mixer_models/sam/Mixer-B_16.npz\n230.04 MiB  2021-06-28T13:08:08Z  gs://mixer_models/sam/Mixer-B_32.npz\nTOTAL: 12 objects, 3489219014 bytes (3.25 GiB)\n\n\n\n# Download a pre-trained model.\n\n# Note: you can really choose any of the above, but this Colab has been tested\n# with the models of below selection...\nmodel_name = 'ViT-B_32'  #@param [\"ViT-B_32\", \"Mixer-B_16\"]\n\nif model_name.startswith('ViT'):\n  ![ -e \"$model_name\".npz ] || gsutil cp gs://vit_models/imagenet21k/\"$model_name\".npz .\nif model_name.startswith('Mixer'):\n  ![ -e \"$model_name\".npz ] || gsutil cp gs://mixer_models/imagenet21k/\"$model_name\".npz .\n\nimport os\nassert os.path.exists(f'{model_name}.npz')\n\nCopying gs://vit_models/imagenet21k/ViT-B_32.npz...\n/ [1 files][400.0 MiB/400.0 MiB]                                                \nOperation completed over 1 objects/400.0 MiB.                                    \n\n\n\n# Google Colab \"TPU\" runtimes are configured in \"2VM mode\", meaning that JAX\n# cannot see the TPUs because they're not directly attached. Instead we need to\n# setup JAX to communicate with a second machine that has the TPUs attached.\nimport os\nif 'google.colab' in str(get_ipython()) and 'COLAB_TPU_ADDR' in os.environ:\n  import jax\n  import jax.tools.colab_tpu\n  jax.tools.colab_tpu.setup_tpu()\n  print('Connected to TPU.')\nelse:\n  print('No TPU detected. Can be changed under \"Runtime/Change runtime type\".')\n\nNo TPU detected. Can be changed under \"Runtime/Change runtime type\".\n\n\n\nfrom absl import logging\nimport flax\nimport jax\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport optax\nimport tqdm\n\nlogging.set_verbosity(logging.INFO)\n\n# Shows the number of available devices.\n# In a CPU/GPU runtime this will be a single device.\n# In a TPU runtime this will be 8 cores.\njax.local_devices()\n\n[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n\n\n\n# Open some code files in a split editor on the right.\n# You can open more files in the file tab on the left.\nfrom google.colab import files\nfiles.view('vision_transformer/vit_jax/configs/common.py')\nfiles.view('vision_transformer/vit_jax/configs/models.py')\nfiles.view('vision_transformer/vit_jax/checkpoint.py')\nfiles.view('vision_transformer/vit_jax/input_pipeline.py')\nfiles.view('vision_transformer/vit_jax/models.py')\nfiles.view('vision_transformer/vit_jax/train.py')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Import files from repository.\n# Updating the files in the editor on the right will immediately update the\n# modules by re-importing them.\n\nimport sys\nif './vision_transformer' not in sys.path:\n  sys.path.append('./vision_transformer')\n\n%load_ext autoreload\n%autoreload 2\n\nfrom vit_jax import checkpoint\nfrom vit_jax import input_pipeline\nfrom vit_jax import utils\nfrom vit_jax import models\nfrom vit_jax import train\nfrom vit_jax.configs import common as common_config\nfrom vit_jax.configs import models as models_config\n\n\n# Helper functions for images.\n\nlabelnames = dict(\n  # https://www.cs.toronto.edu/~kriz/cifar.html\n  cifar10=('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'),\n  # https://www.cs.toronto.edu/~kriz/cifar.html\n  cifar100=('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'computer_keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')\n)\ndef make_label_getter(dataset):\n  \"\"\"Returns a function converting label indices to names.\"\"\"\n  def getter(label):\n    if dataset in labelnames:\n      return labelnames[dataset][label]\n    return f'label={label}'\n  return getter\n\ndef show_img(img, ax=None, title=None):\n  \"\"\"Shows a single image.\"\"\"\n  if ax is None:\n    ax = plt.gca()\n  ax.imshow(img[...])\n  ax.set_xticks([])\n  ax.set_yticks([])\n  if title:\n    ax.set_title(title)\n\ndef show_img_grid(imgs, titles):\n  \"\"\"Shows a grid of images.\"\"\"\n  n = int(np.ceil(len(imgs)**.5))\n  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n  for i, (img, title) in enumerate(zip(imgs, titles)):\n    img = (img + 1) / 2  # Denormalize\n    show_img(img, axs[i // n][i % n], title)\n\n\n\n10.0.3 Load dataset\n\ndataset = 'cifar10'\nbatch_size = 512\nconfig = common_config.with_dataset(common_config.get_config(), dataset)\nconfig.batch = batch_size\nconfig.pp.crop = 224\n\n\n# For details about setting up datasets, see input_pipeline.py on the right.\nds_train = input_pipeline.get_data_from_tfds(config=config, mode='train')\nds_test = input_pipeline.get_data_from_tfds(config=config, mode='test')\nnum_classes = input_pipeline.get_dataset_info(dataset, 'train')['num_classes']\ndel config  # Only needed to instantiate datasets.\n\nINFO:absl:Generating dataset cifar10 (/root/tensorflow_datasets/cifar10/3.0.2)\n\n\nDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/cifar10/3.0.2...\n\n\n\n\n\n\n\n\n\n\n\nINFO:absl:Downloading https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz into /root/tensorflow_datasets/downloads/cs.toronto.edu_kriz_cifar-10-binaryODHPtIjLh3oLcXirEISTO7dkzyKjRCuol6lV8Wc6C7s.tar.gz.tmp.f59c0efb72c04101895b5182067b680f...\n\n\n\n\n\n\n\n\n\n\n\nINFO:absl:Done writing /root/tensorflow_datasets/cifar10/3.0.2.incompleteR9LONN/cifar10-train.tfrecord*. Number of examples: 50000 (shards: [50000])\n\n\n\n\n\n\n\n\nINFO:absl:Done writing /root/tensorflow_datasets/cifar10/3.0.2.incompleteR9LONN/cifar10-test.tfrecord*. Number of examples: 10000 (shards: [10000])\nINFO:absl:Constructing tf.data.Dataset cifar10 for split train[:98%], from /root/tensorflow_datasets/cifar10/3.0.2\nINFO:absl:Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\n\n\nDataset cifar10 downloaded and prepared to /root/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\n\n\nINFO:absl:Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\nINFO:absl:Reusing dataset cifar10 (/root/tensorflow_datasets/cifar10/3.0.2)\nINFO:absl:Constructing tf.data.Dataset cifar10 for split test, from /root/tensorflow_datasets/cifar10/3.0.2\nINFO:absl:Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\nINFO:absl:Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\n\n\n\n# Fetch a batch of test images for illustration purposes.\nbatch = next(iter(ds_test.as_numpy_iterator()))\n# Note the shape : [num_local_devices, local_batch_size, h, w, c]\nbatch['image'].shape\n\n(1, 512, 224, 224, 3)\n\n\n\n# Show some images with their labels.\nimages, labels = batch['image'][0][:9], batch['label'][0][:9]\ntitles = map(make_label_getter(dataset), labels.argmax(axis=1))\nshow_img_grid(images, titles)\n\n\n\n\n\n# Same as above, but with train images.\n# Note how images are cropped/scaled differently.\n# Check out input_pipeline.get_data() in the editor at your right to see how the\n# images are preprocessed differently.\nbatch = next(iter(ds_train.as_numpy_iterator()))\nimages, labels = batch['image'][0][:9], batch['label'][0][:9]\ntitles = map(make_label_getter(dataset), labels.argmax(axis=1))\nshow_img_grid(images, titles)\n\n\n\n\n\n\n10.0.4 Load pre-trained\n\nmodel_config = models_config.MODEL_CONFIGS[model_name]\nmodel_config\n\nclassifier: token\nhidden_size: 768\nmodel_name: ViT-B_32\npatches:\n  size: !!python/tuple\n  - 32\n  - 32\nrepresentation_size: null\ntransformer:\n  attention_dropout_rate: 0.0\n  dropout_rate: 0.0\n  mlp_dim: 3072\n  num_heads: 12\n  num_layers: 12\n\n\n\n# Load model definition & initialize random parameters.\n# This also compiles the model to XLA (takes some minutes the first time).\nif model_name.startswith('Mixer'):\n  model = models.MlpMixer(num_classes=num_classes, **model_config)\nelse:\n  model = models.VisionTransformer(num_classes=num_classes, **model_config)\nvariables = jax.jit(lambda: model.init(\n    jax.random.PRNGKey(0),\n    # Discard the \"num_local_devices\" dimension of the batch for initialization.\n    batch['image'][0, :1],\n    train=False,\n), backend='cpu')()\n\n\n# Load and convert pretrained checkpoint.\n# This involves loading the actual pre-trained model results, but then also also\n# modifying the parameters a bit, e.g. changing the final layers, and resizing\n# the positional embeddings.\n# For details, refer to the code and to the methods of the paper.\nparams = checkpoint.load_pretrained(\n    pretrained_path=f'{model_name}.npz',\n    init_params=variables['params'],\n    model_config=model_config,\n)\n\nINFO:absl:Inspect extra keys:\n{'pre_logits/bias', 'pre_logits/kernel'}\nINFO:absl:load_pretrained: drop-head variant\n\n\n\n\n10.0.5 Evaluate\n\n# So far, all our data is in the host memory. Let's now replicate the arrays\n# into the devices.\n# This will make every array in the pytree params become a ShardedDeviceArray\n# that has the same data replicated across all local devices.\n# For TPU it replicates the params in every core.\n# For a single GPU this simply moves the data onto the device.\n# For CPU it simply creates a copy.\nparams_repl = flax.jax_utils.replicate(params)\nprint('params.cls:', type(params['head']['bias']).__name__,\n      params['head']['bias'].shape)\nprint('params_repl.cls:', type(params_repl['head']['bias']).__name__,\n      params_repl['head']['bias'].shape)\n\nparams.cls: ArrayImpl (10,)\nparams_repl.cls: ArrayImpl (1, 10)\n\n\n\n# Then map the call to our model's forward pass onto all available devices.\nvit_apply_repl = jax.pmap(lambda params, inputs: model.apply(\n    dict(params=params), inputs, train=False))\n\n\ndef get_accuracy(params_repl):\n  \"\"\"Returns accuracy evaluated on the test set.\"\"\"\n  good = total = 0\n  steps = input_pipeline.get_dataset_info(dataset, 'test')['num_examples'] // batch_size\n  for _, batch in zip(tqdm.trange(steps), ds_test.as_numpy_iterator()):\n    predicted = vit_apply_repl(params_repl, batch['image'])\n    is_same = predicted.argmax(axis=-1) == batch['label'].argmax(axis=-1)\n    good += is_same.sum()\n    total += len(is_same.flatten())\n  return good / total\n\n\n# Random performance without fine-tuning.\nget_accuracy(params_repl)\n\nINFO:absl:Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\n100%|██████████| 19/19 [00:42&lt;00:00,  2.23s/it]\n\n\nArray(0.10063734, dtype=float32)\n\n\n\n\n10.0.6 Fine-tune\n\n# 100 Steps take approximately 15 minutes in the TPU runtime.\ntotal_steps = 100\nwarmup_steps = 5\ndecay_type = 'cosine'\ngrad_norm_clip = 1\n# This controls in how many forward passes the batch is split. 8 works well with\n# a TPU runtime that has 8 devices. 64 should work on a GPU. You can of course\n# also adjust the batch_size above, but that would require you to adjust the\n# learning rate accordingly.\naccum_steps = 8\nbase_lr = 0.03\n\n\n# Check out train.make_update_fn in the editor on the right side for details.\nlr_fn = utils.create_learning_rate_schedule(total_steps, base_lr, decay_type, warmup_steps)\n# We use a momentum optimizer that uses half precision for state to save\n# memory. It als implements the gradient clipping.\ntx = optax.chain(\n    optax.clip_by_global_norm(grad_norm_clip),\n    optax.sgd(\n        learning_rate=lr_fn,\n        momentum=0.9,\n        accumulator_dtype='bfloat16',\n    ),\n)\nupdate_fn_repl = train.make_update_fn(\n    apply_fn=model.apply, accum_steps=accum_steps, tx=tx)\nopt_state = tx.init(params)\nopt_state_repl = flax.jax_utils.replicate(opt_state)\n\n\n# Initialize PRNGs for dropout.\nupdate_rng_repl = flax.jax_utils.replicate(jax.random.PRNGKey(0))\n\n\nlosses = []\nlrs = []\n# Completes in ~20 min on the TPU runtime.\nfor step, batch in zip(\n    tqdm.trange(1, total_steps + 1),\n    ds_train.as_numpy_iterator(),\n):\n\n  params_repl, opt_state_repl, loss_repl, update_rng_repl = update_fn_repl(\n      params_repl, opt_state_repl, batch, update_rng_repl)\n  losses.append(loss_repl[0])\n  lrs.append(lr_fn(step))\n\nplt.plot(losses)\nplt.figure()\nplt.plot(lrs)\n\n100%|██████████| 100/100 [07:22&lt;00:00,  4.43s/it]\n\n\n\n\n\n\n\n\n\n# Should be ~96.7% for Mixer-B/16 or 97.7% for ViT-B/32 on CIFAR10 (both @224)\nget_accuracy(params_repl)\n\nINFO:absl:Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\n100%|██████████| 19/19 [00:28&lt;00:00,  1.48s/it]\n\n\nArray(0.9763569, dtype=float32)\n\n\n\n\n10.0.7 Inference\n\n# Download a pre-trained model.\n\nif model_name.startswith('Mixer'):\n  # Download model trained on imagenet2012\n  ![ -e \"$model_name\"_imagenet2012.npz ] || gsutil cp gs://mixer_models/imagenet1k/\"$model_name\".npz \"$model_name\"_imagenet2012.npz\n  model = models.MlpMixer(num_classes=1000, **model_config)\nelse:\n  # Download model pre-trained on imagenet21k and fine-tuned on imagenet2012.\n  ![ -e \"$model_name\"_imagenet2012.npz ] || gsutil cp gs://vit_models/imagenet21k+imagenet2012/\"$model_name\".npz \"$model_name\"_imagenet2012.npz\n  model = models.VisionTransformer(num_classes=1000, **model_config)\n\nimport os\nassert os.path.exists(f'{model_name}_imagenet2012.npz')\n\nCopying gs://vit_models/imagenet21k/ViT-B_32.npz...\n/ [1 files][400.0 MiB/400.0 MiB]                                                \nOperation completed over 1 objects/400.0 MiB.                                    \n\n\n\n# Load and convert pretrained checkpoint.\nparams = checkpoint.load(f'{model_name}_imagenet2012.npz')\nparams['pre_logits'] = {}  # Need to restore empty leaf for Flax.\n\n\n# Get imagenet labels.\n!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\nimagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))\n\n--2023-05-04 02:19:16--  https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\nResolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 172.217.218.128, 142.251.18.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21675 (21K) [text/plain]\nSaving to: ‘ilsvrc2012_wordnet_lemmas.txt’\n\nilsvrc2012_wordnet_ 100%[===================&gt;]  21.17K  --.-KB/s    in 0s      \n\n2023-05-04 02:19:16 (123 MB/s) - ‘ilsvrc2012_wordnet_lemmas.txt’ saved [21675/21675]\n\n\n\n\n# Get a random picture with the correct dimensions.\nresolution = 224 if model_name.startswith('Mixer') else 384\n!wget https://picsum.photos/$resolution -O picsum.jpg\nimport PIL\nimg = PIL.Image.open('picsum.jpg')\nimg\n\n--2023-05-04 02:25:59--  https://picsum.photos/384\nResolving picsum.photos (picsum.photos)... 104.26.4.30, 104.26.5.30, 172.67.74.163, ...\nConnecting to picsum.photos (picsum.photos)|104.26.4.30|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://fastly.picsum.photos/id/541/384/384.jpg?hmac=9ExXyH6DwxuYbmyHKfjMKmpZBdyR8aeyOdjarCvCeNg [following]\n--2023-05-04 02:25:59--  https://fastly.picsum.photos/id/541/384/384.jpg?hmac=9ExXyH6DwxuYbmyHKfjMKmpZBdyR8aeyOdjarCvCeNg\nResolving fastly.picsum.photos (fastly.picsum.photos)... 151.101.1.91, 151.101.65.91, 151.101.129.91, ...\nConnecting to fastly.picsum.photos (fastly.picsum.photos)|151.101.1.91|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10165 (9.9K) [image/jpeg]\nSaving to: ‘picsum.jpg’\n\npicsum.jpg          100%[===================&gt;]   9.93K  --.-KB/s    in 0s      \n\n2023-05-04 02:26:00 (76.9 MB/s) - ‘picsum.jpg’ saved [10165/10165]\n\n\n\n\n\n\n\n# Predict on a batch with a single item (note very efficient TPU usage...)\nlogits, = model.apply(dict(params=params), (np.array(img) / 128 - 1)[None, ...], train=False)\n\n\npreds = np.array(jax.nn.softmax(logits))\nfor idx in preds.argsort()[:-11:-1]:\n  print(f'{preds[idx]:.5f} : {imagenet_labels[idx]}', end='')\n\n0.98121 : scuba_diver\n0.00279 : snorkel\n0.00274 : coral_reef\n0.00215 : wreck\n0.00156 : tiger_shark, Galeocerdo_cuvieri\n0.00150 : sea_snake\n0.00070 : lionfish\n0.00062 : electric_ray, crampfish, numbfish, torpedo\n0.00047 : hammerhead, hammerhead_shark\n0.00035 : great_white_shark, white_shark, man-eater, man-eating_shark, Carcharodon_carcharias"
  },
  {
    "objectID": "exercise1_jax_mnist_solution.html",
    "href": "exercise1_jax_mnist_solution.html",
    "title": "11  Exercise 1 Solution",
    "section": "",
    "text": "With thanks to DeepMind for code here.\n\n# Copyright 2018 The JAX Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A basic MNIST example using Numpy and JAX.\n\nThe primary aim here is simplicity and minimal dependencies.\n\"\"\"\n\nimport array\nimport gzip\nimport os\nfrom os import path\nimport struct\nimport urllib.request\n\nimport numpy as np\n\n\n_DATA = \"/tmp/jax_example_data/\"\n\n\ndef _download(url, filename):\n  \"\"\"Download a url to a file in the JAX data temp directory.\"\"\"\n  if not path.exists(_DATA):\n    os.makedirs(_DATA)\n  out_file = path.join(_DATA, filename)\n  if not path.isfile(out_file):\n    urllib.request.urlretrieve(url, out_file)\n    print(f\"downloaded {url} to {_DATA}\")\n\n\ndef _partial_flatten(x):\n  \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n  return np.reshape(x, (x.shape[0], -1))\n\n\ndef _one_hot(x, k, dtype=np.float32):\n  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n  return np.array(x[:, None] == np.arange(k), dtype)\n\n\ndef mnist_raw():\n  \"\"\"Download and parse the raw MNIST dataset.\"\"\"\n  # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n  base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n\n  def parse_labels(filename):\n    with gzip.open(filename, \"rb\") as fh:\n      _ = struct.unpack(\"&gt;II\", fh.read(8))\n      return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n\n  def parse_images(filename):\n    with gzip.open(filename, \"rb\") as fh:\n      _, num_data, rows, cols = struct.unpack(\"&gt;IIII\", fh.read(16))\n      return np.array(array.array(\"B\", fh.read()),\n                      dtype=np.uint8).reshape(num_data, rows, cols)\n\n  for filename in [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n                   \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]:\n    _download(base_url + filename, filename)\n\n  train_images = parse_images(path.join(_DATA, \"train-images-idx3-ubyte.gz\"))\n  train_labels = parse_labels(path.join(_DATA, \"train-labels-idx1-ubyte.gz\"))\n  test_images = parse_images(path.join(_DATA, \"t10k-images-idx3-ubyte.gz\"))\n  test_labels = parse_labels(path.join(_DATA, \"t10k-labels-idx1-ubyte.gz\"))\n\n  return train_images, train_labels, test_images, test_labels\n\n\ndef mnist(permute_train=False):\n  \"\"\"Download, parse and process MNIST data to unit scale and one-hot labels.\"\"\"\n  train_images, train_labels, test_images, test_labels = mnist_raw()\n\n  train_images = _partial_flatten(train_images) / np.float32(255.)\n  test_images = _partial_flatten(test_images) / np.float32(255.)\n  train_labels = _one_hot(train_labels, 10)\n  test_labels = _one_hot(test_labels, 10)\n\n  if permute_train:\n    perm = np.random.RandomState(0).permutation(train_images.shape[0])\n    train_images = train_images[perm]\n    train_labels = train_labels[perm]\n\n  return train_images, train_labels, test_images, test_labels\n\n\n# Copyright 2018 The JAX Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"A basic MNIST example using Numpy and JAX.\n\nThe primary aim here is simplicity and minimal dependencies.\n\"\"\"\n\n\nimport time\n\nimport numpy.random as npr\n\nfrom jax import jit, grad\nfrom jax.scipy.special import logsumexp\nimport jax.numpy as jnp\nfrom examples import datasets\n\ndef init_random_params(scale, layer_sizes):\n    # Solution\n    key = random.PRNGKey(0)\n    # Split the PRNGKey into two new keys\n    key1, key2 = random.split(key, 2)\n    params = [(scale * random.normal(key1, (m, n)), scale * random.normal(key2, (n,)))\n              for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]\n    return params\n\n\ndef predict(params, inputs):\n  activations = inputs\n  for w, b in params[:-1]:\n    outputs = jnp.dot(activations, w) + b\n    activations = jnp.tanh(outputs)\n\n  final_w, final_b = params[-1]\n  logits = jnp.dot(activations, final_w) + final_b\n  return logits - logsumexp(logits, axis=1, keepdims=True)\n\ndef loss(params, batch):\n  inputs, targets = batch\n  preds = predict(params, inputs)\n  return -jnp.mean(jnp.sum(preds * targets, axis=1))\n\ndef accuracy(params, batch):\n  inputs, targets = batch\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\n\nif __name__ == \"__main__\":\n  layer_sizes = [784, 1024, 1024, 10]\n  param_scale = 0.1\n  step_size = 0.001\n  num_epochs = 10\n  batch_size = 128\n\n  train_images, train_labels, test_images, test_labels = mnist()\n  num_train = train_images.shape[0]\n  num_complete_batches, leftover = divmod(num_train, batch_size)\n  num_batches = num_complete_batches + bool(leftover)\n\n  def data_stream():\n    rng = npr.RandomState(0)\n    while True:\n      perm = rng.permutation(num_train)\n      for i in range(num_batches):\n        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n        yield train_images[batch_idx], train_labels[batch_idx]\n  batches = data_stream()\n\n  # Solution\n  # jit compiling the update function brings the most benefits;\n  # it does the heavy lifting for the training loop and runs\n  # many times\n  @jit\n  def update(params, batch):\n    # Solution\n    grads = grad(loss)(params, batch)\n    return [(w - step_size * dw, b - step_size * db)\n            for (w, b), (dw, db) in zip(params, grads)]\n\n  params = init_random_params(param_scale, layer_sizes)\n  for epoch in range(num_epochs):\n    start_time = time.time()\n    for _ in range(num_batches):\n      params = update(params, next(batches))\n    epoch_time = time.time() - start_time\n\n    train_acc = accuracy(params, (train_images, train_labels))\n    test_acc = accuracy(params, (test_images, test_labels))\n    print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n    print(f\"Training set accuracy {train_acc}\")\n    print(f\"Test set accuracy {test_acc}\")"
  },
  {
    "objectID": "exercise2_solution.html",
    "href": "exercise2_solution.html",
    "title": "12  Exercise 2 Solution",
    "section": "",
    "text": "import jax\nfrom jax import numpy as jnp, random, lax, jit\nfrom flax import linen as nn\n\nX = jnp.ones((1, 10))\nY = jnp.ones((5,))\n\nmodel = nn.Dense(features=5)\n\n@jit\ndef predict(params):\n  return model.apply({'params': params}, X)\n\n@jit\ndef loss_fn(params):\n  return jnp.mean(jnp.abs(Y - predict(params)))\n\n@jit\ndef init_params(rng):\n  mlp_variables = model.init({'params': rng}, X)\n  return mlp_variables['params']\n\n# Get initial parameters\nparams = init_params(jax.random.PRNGKey(42))\nprint(\"initial params\", params)\n\n# Run SGD.\nfor i in range(50):\n  loss, grad = jax.value_and_grad(loss_fn)(params)\n  print(i, \"loss = \", loss, \"Yhat = \", predict(params))\n  lr = 0.03\n  params = jax.tree_util.tree_map(lambda x, d: x - lr * d, params, grad)"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13  Resources",
    "section": "",
    "text": "JAX docs\nFlax docs\nHuggingFace diffusers library\nGoogle Research VIT\nJAX on GKE Example (GPU)"
  }
]