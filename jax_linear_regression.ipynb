{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685082691770,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "NstEPePmvSSB"
   },
   "source": [
    "### Linear Regression in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import tree_util, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/rastringer/jax_notebooks/blob/master/jax_linear_regression.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao4ISl8001ET"
   },
   "source": [
    "Reference: [JAX for the impatient](https://flax.readthedocs.io/en/latest/guides/jax_for_the_impatient.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-K2J28ox3-Z"
   },
   "source": [
    "### Pytrees\n",
    "\n",
    "Before we jump into linear regression in JAX, let's have a quick look at pytrees. Pytrees are everywhere in JAX and Flax.\n",
    "\n",
    "JAX treats a pytree as a container of leaf elements. These can include lists, tuples and dicts, so is basically a structure for nested data. Container types do not need to match if nested.\n",
    "\n",
    "JAX provides the `tree_util` package for working with pytrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1685083308622,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "qaLfL7-XygTn",
    "outputId": "8a9f4b9b-1b76-4227-e09c-8eefb05fae92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: [1, {'k1': 2, 'k2': (3, 4)}, 5]\n",
      "tree_util.tree_map(lambda x: x*2, tree): [2, {'k1': 4, 'k2': (6, 8)}, 10]\n"
     ]
    }
   ],
   "source": [
    "from jax import tree_util\n",
    "\n",
    "tree = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\n",
    "print('tree:', tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20M0bTUWyuzM"
   },
   "source": [
    "The `tree_map` function is frequently used for updating a tree and its leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1685083381087,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "-PXQEjP0y6xS",
    "outputId": "0ddc5aff-f364-41d4-98e3-605916aa6d7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, {'k1': 4, 'k2': (6, 8)}, 10]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_map(lambda x: x*2, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bAfPXJdzAK7"
   },
   "source": [
    "We can also provide a tuple of aditional trees of the same shape to the original tree to enable a function to operate on each leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685083515927,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "VC8QiYK3zPYb",
    "outputId": "264f01c1-ede2-429c-c893-a9595c6bc0ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, {'k1': 6, 'k2': (9, 12)}, 15]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_tree = tree_util.tree_map(lambda x: x*2, tree)\n",
    "tree_util.tree_map(lambda x,y: x+y, tree, transformed_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685082795844,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "DYaJxeOswueI"
   },
   "outputs": [],
   "source": [
    "# Linear feed-forward.\n",
    "def predict(W, b, x):\n",
    "  return jnp.dot(x, W) + b\n",
    "\n",
    "# Loss function: Mean squared error.\n",
    "def mse(W, b, x_batched, y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x, y):\n",
    "    y_pred = predict(W, b, x)\n",
    "    return jnp.inner(y-y_pred, y-y_pred) / 2.0\n",
    "  # We vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1685082797287,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "2a1_5HY7vjCu",
    "outputId": "afe94ed5-834f-4b02-e8e2-71affc79aac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = predict(W, b, x_samples) + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzXJWXMUzmbb"
   },
   "source": [
    "In this linear regression, `params` is a pytree which contains `W` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685082799292,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "A2OK-WetwjoZ"
   },
   "outputs": [],
   "source": [
    "# Linear feed-forward that takes a params pytree.\n",
    "def predict_pytree(params, x):\n",
    "  return jnp.dot(x, params['W']) + params['b']\n",
    "\n",
    "# Loss function: Mean squared error.\n",
    "def mse_pytree(params, x_batched,y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x,y):\n",
    "    y_pred = predict_pytree(params, x)\n",
    "    return jnp.inner(y-y_pred, y-y_pred) / 2.0\n",
    "  # We vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n",
    "\n",
    "# Initialize estimated W and b with zeros. Store in a pytree.\n",
    "params = {'W': jnp.zeros_like(W), 'b': jnp.zeros_like(b)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVz2DqlAz5-c"
   },
   "source": [
    "JAX can differentiate the pytree parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1685083624708,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "uNeYVmlyz4rt",
    "outputId": "abb569c6-ba95-42d2-dc97-eaa461dfb5f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': Array([[ 3.02512199e-05,  2.38317996e-04,  5.86672686e-05,\n",
       "          1.45167112e-04, -1.08840875e-04],\n",
       "        [-7.21593387e-05, -5.92094846e-04, -1.44919526e-04,\n",
       "         -3.55741940e-04,  2.12500338e-04],\n",
       "        [ 6.48805872e-06,  5.42663038e-05,  1.30501576e-05,\n",
       "          3.26364461e-05, -1.73687004e-05],\n",
       "        [-1.58965122e-05, -1.67803839e-04, -3.67928296e-05,\n",
       "         -9.91356210e-05,  1.37500465e-05],\n",
       "        [-9.83832870e-05, -7.85302604e-04, -1.94110908e-04,\n",
       "         -4.74306522e-04,  3.20815481e-04],\n",
       "        [-6.23832457e-05, -5.39575703e-04, -1.28836837e-04,\n",
       "         -3.22562526e-04,  1.54131034e-04],\n",
       "        [-8.21873546e-05, -6.98693097e-04, -1.67359249e-04,\n",
       "         -4.20103315e-04,  2.30040867e-04],\n",
       "        [-7.44033605e-06, -5.03805932e-05, -1.21158082e-05,\n",
       "         -3.23755667e-05,  4.07989137e-05],\n",
       "        [ 2.81375833e-06,  5.13494015e-05,  9.00402665e-06,\n",
       "          2.99257226e-05,  1.63719524e-05],\n",
       "        [-2.90344469e-05, -2.14974396e-04, -5.49759716e-05,\n",
       "         -1.30489469e-04,  1.08879060e-04]], dtype=float32),\n",
       " 'b': Array([-3.0185096e-05, -2.2265501e-04, -5.7548052e-05, -1.3502731e-04,\n",
       "         1.1305924e-04], dtype=float32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(mse_pytree)(params, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1685082801178,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "Zn54D9oyvX1X",
    "outputId": "23b1c3b7-385a-40cf-e476-291c7e65aac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.02363979\n",
      "Loss step 0:  10.97141\n",
      "Loss step 5:  1.0798324\n",
      "Loss step 10:  0.3795825\n",
      "Loss step 15:  0.17855297\n",
      "Loss step 20:  0.094415195\n",
      "Loss step 25:  0.054522194\n",
      "Loss step 30:  0.03448924\n",
      "Loss step 35:  0.024058029\n",
      "Loss step 40:  0.018480862\n",
      "Loss step 45:  0.015438682\n",
      "Loss step 50:  0.01375394\n",
      "Loss step 55:  0.0128103\n",
      "Loss step 60:  0.012277315\n",
      "Loss step 65:  0.011974388\n",
      "Loss step 70:  0.011801446\n",
      "Loss step 75:  0.011702419\n",
      "Loss step 80:  0.011645543\n",
      "Loss step 85:  0.011612838\n",
      "Loss step 90:  0.011594015\n",
      "Loss step 95:  0.011583163\n",
      "Loss step 100:  0.011576912\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def update_params_pytree(params, learning_rate, x_samples, y_samples):\n",
    "  params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params,\n",
    "        jax.grad(mse_pytree)(params, x_samples, y_samples))\n",
    "  return params\n",
    "\n",
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse_pytree({'W': W, 'b': b}, x_samples, y_samples))\n",
    "for i in range(101):\n",
    "  # Perform one gradient update.\n",
    "  params = update_params_pytree(params, learning_rate, x_samples, y_samples)\n",
    "  if (i % 5 == 0):\n",
    "    print(f\"Loss step {i}: \", mse_pytree(params, x_samples, y_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50wxZzd30Gf7"
   },
   "source": [
    "Here we can also use `jax.value_and_grad()` to compute both the return value of the input function and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1685083675807,
     "user": {
      "displayName": "Robin Stringer",
      "userId": "05796722230835218202"
     },
     "user_tz": -60
    },
    "id": "fdA_Jf86wc_w",
    "outputId": "2c8c5dd8-fb1e-4779-a190-ad45d15d840f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.011576912\n",
      "Loss step 5:  0.011573299\n",
      "Loss step 10:  0.011571216\n",
      "Loss step 15:  0.011570027\n",
      "Loss step 20:  0.0115693165\n",
      "Loss step 25:  0.011568918\n",
      "Loss step 30:  0.011568695\n",
      "Loss step 35:  0.01156855\n",
      "Loss step 40:  0.011568478\n",
      "Loss step 45:  0.011568436\n",
      "Loss step 50:  0.011568408\n",
      "Loss step 55:  0.011568391\n",
      "Loss step 60:  0.01156838\n",
      "Loss step 65:  0.011568381\n",
      "Loss step 70:  0.01156838\n",
      "Loss step 75:  0.011568385\n",
      "Loss step 80:  0.011568374\n",
      "Loss step 85:  0.01156838\n",
      "Loss step 90:  0.01156837\n",
      "Loss step 95:  0.0115683675\n",
      "Loss step 100:  0.01156837\n"
     ]
    }
   ],
   "source": [
    "# Using jax.value_and_grad instead:\n",
    "loss_grad_fn = jax.value_and_grad(mse_pytree)\n",
    "for i in range(101):\n",
    "  # Note that here the loss is computed before the param update.\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, grads)\n",
    "    if (i % 5 == 0):\n",
    "        print(f\"Loss step {i}: \", loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EA2-fux0FU8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPOztAMKIjJ7QIIVCVvkBOY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
